---
title: Provisioning an NSX-T Load Balancer for the TKGI API Server
owner: PKS-NSXT
---

This topic describes how to deploy an NSX-T load balancer for the <%= vars.product_short %> API Server.

##<a id='about'></a> About the NSX-T Load Balancer for the TKGI API Server

NSX-T provides a converged management and control plane that is referred to as the **NSX-T Management Cluster**. The architecture delivers high availability of the NSX-T Manager node, reduces the likelihood of operation failures of NSX-T, and provides API and UI clients with multiple endpoints or a single VIP for high availability.

To provision an NSX-T load balancer for the TKGI API Server VM, complete the following steps.

##<a id='create-nsgroup'></a> Step 1: Create NSGroup

If you are using a Dynamic Server Pool, create an NSGroup. If you are using a Static Server Pool, skip this step.

1. Log in to an NSX-T Manager Node.
<p class="note"><strong>Note</strong>: You can connect to any NSX-T Manager Node in the management cluster to provision the load balancer.</p>
1. Select the **Advanced Networking & Security** tab.
<p class="note"><strong>Note</strong>: You must use the <strong>Advanced Networking and Security</strong> tab in NSX-T Manager to create, read, update, and delete all NSX-T networking objects used for <%= vars.product_short %>.</p>
1. Select **Inventory > Groups**.
1. Click **+ADD** to add an new NSGroup.
1. Enter a name for the NSGroup, for example `tkgi-api`.
1. Click **ADD**.

  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-01.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-02.png" width="425">

##<a id='create-virtual-servers'></a> Step 2: Create Two Virtual Servers

The TKGI API Sever virtual machine hosts two server processes and exposes two ports: the TKGI API Server on port 9021, and the UAA server on port 8443. Each NSX-T Virtual Server listens on one port. Thus, you need two Virtual Servers, one for the TKGI API server and the other for UAA.

###<a id='create-vs-api'></a> Create a Virtual Server for the TKGI API Server

1. In NSX Manager, select **Networking > Load Balancing > Virtual Servers**.
1. Click Add.
1. Configure General Properties for the Virtual Server.
  - Name: `tkgi-api-server`, for example
  - Application Types: Choose **Layer 7 TCP**. Or, you can choose Layer 4, in which case you don't have to configure SSL for the virtual server.
  - Application Profile: `default-http-lb-app-profile`
  - Access Log: Disabled
  - Click **Next**
1. Configure Virtual Server Identifiers.
  - IP Address: Enter an IP address from the floating pool, such as `192/168.160.108`
  - Port: **9021** (for the TKGS API Server)
  - Click **Next**
1. Configure Server Pool and Rules.
  - Click **Create a New Server Pool**
  - Name the server pool, for example **tkgi-api-server**
  - Load Balancing Algorithm: **ROUND_ROBIN** (for example)
  - Click **Next**
1. Configure SNAT Translation for the Server Pool:
  - Translation Mode: Auto Map
  - Click **Next**
1. OPTION 1: Configure Pool Members for the Static Server Pool:
  - Membership Type: **Static**
  - Leave members empty. This will be added automatically later when you apply changes in Ops Manager.
  - Click **Next**
1. OPTION 2: Configure Pool Members for the Dynamic Server Pool:
  - Membership Type: **Dynamic**
  - Set NSGroup as the NSGroup name created in Step 1, such as **tkgi-api**
  - Set Max Group IP Address List to 3, since we can only have up to 3 pks api instances
  - Click **Next**
1. Configure Health Monitors for the virtual server:
  - Click **Create A New Active Monitor**
  - Set **Name**, for example **tkgi-api-server**
  - Set Health Check Protocol **LBHttpsMonitor**
  - Set the port as **9021**
  - Leave other fields as default
  - Click **Next**
1. Configure Health Check Parameters:
  - Choose **High Security for SSL Ciphers**
  - Select **TLS_V1_2** as SSL Protocols
  - Set HTTP Method as **GET**
  - Set HTTP Request URL as **/actuator/health**
  - Set HTTP Request Header as **200**
  - Click **Finish**
1. Configure **New Server Pool > Health Monitors**:
  - Set Active Health Monitor as the what was created: **tkgi-api-server**
  - Click **Finish**
1. Configure the Virtual Server:
  - Set the Default Server Pool to what you just created: **tkgi-api-server**
  - Click **Next**
1. Persistence Profiles is optional. Click **Next**.
1. Configure Client Side SSL (only if L7 Application Type is selected). Use the default certificates. Click **Next**.
1. Configure Server Side SSL(only if L7 Application Type is selected). Use the default certificates. Click **Finish**.

  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-03.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-04.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-05.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-06.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-07.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-08.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-09.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-10.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-11.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-12.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-13.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-14.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-15.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-16.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-17.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-18.png" width="425">

###<a id='create-vs-uaa'></a> Create a Virtual Server for the UAA Server

1. Select the tkgs-api-server Virtual Server you created and and click **Clone**.
1. Click **Edit** to change the content and configure the virtual server for UAA.
1. Configure General Properties
  - Set the Name as **tkgi-api-uaa**
  - Click **Next**
1. Configure Virtual Server Identifiers
  - Set the Port to **8443**
  - Click **Next**
1. Configure Server Pool and Rules
  - Click Create A New Server Pool
  - Set name to **tkgi-api-uaa**
  - Click **Next**
1. Configure SNAT Translation for the Server Pool.
  - Pool Members should be the same that you configured for the tkgi-api vritual server
  - Click **Next**
1. Configure Health Monitors:
  - Click Create A New Active Monitor
  - Set Name, for example **tkgi-api-uaa**
  - Set Health Check Protocol **LBHttpsMonitor**
  - Set port as **8443**
  - Leave other fields as default
  - Click **Next**
1. Configure Health Check Parameters:
  - Choose High Security for SSL Ciphers
  - Select TLS_V1_2 as SSL Protocols
  - Set HTTP Request URL as /healthz
  - Set HTTP Request Header as 200
  - Click **Finish**
1. Configure a New Server Pool
  - Set Active Health Monitor to pks-api-uaa
  - Click **Finish**
1. Edit Virtual Server
  - Set Default Server Pool as pks-api-uaa
  - Click **Next**
1. Persistence Profiles is optional. Click **Next**.
1. Configure Client Side SSL (only if L7 Application Type is selected). Use the default certificates. Click **Next**.
1. Configure Server Side SSL(only if L7 Application Type is selected). Use the default certificates. Click **Finish**. 

  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-19.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-20.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-21.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-22.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-23.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-24.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-25.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-26.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-27.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-28.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-29.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-30.png" width="425">

##<a id='create-lb'></a> Step 3: Create Load Balancer

1. In NSX Manager, select Networking > Load Balancing > Load Balancers.
1. Click Add.
1. Set the name: for example **tkgi-api**
1. Choose the Load Balancer Size. The default SMALL should be sufficient for most TKGI deployments. For large-scale deployments, use are larger size load balancer.
1. Click OK.

  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-31.png" width="425">
  <img src="images/nsxt/api-lb/nsxt-lb-tkgi-api-32.png" width="425">


###<a id='attach-lb'></a> Step 4: Attach the Load Balancer to a Logical Router Gateway

1. In NSX Manager, select Networking > Load Balancing > Load Balancers.
1. Choose the tkgs-api load balancer you just created.
1. Click Attach to a Logical Router
1. Choose a Tier-1 logical router that is attached to pks api VMs
 
If you see Errors like :
The logical router LogicalRouter/b0a65a53-8004-4d2b-8551-6bc29abc37ca does not have associated edge cluster to deploy a load balancer service LoadBalancerService/c0c478b2-67f2-404d-b109-d72e5e3a5672.

Then you need to create Logical Router that associated to a edge cluster
Optional For troubleshooting


 


●	Inside NSX Manager UI: Manager on Top Right > Networking > Tier-1 Logical Routers
●	Click Add


 

Configure Tier-1 Router
●	Set the name, for example pks-api
●	Set Tier-0 Router
●	Set Edge Cluster
●	Set Edge Cluster member
●	Click Add


 



Configure Route Advertisement for the Tier-1 Router.
●	Select the Tier-1 Router.
●	Select the Routing tab.
●	Select Route Advertisement > Edit.
●	Enable Route Advertisement for all load balancer VIP routes for the Tier-1 Router:
○	Status: enabled
○	Advertise all LB VIP routes: yes
○	Advertise all LB SNAT IP routes: yes
○	Click Save
 
 



 



Attach the Logical Router to Load Balancer 
●	Click OK


###<a id='s5'></a> Step 5: Verify Router and Switch Configuration

Verify successful creation and configuration of the logical switch and router.

- Select the Tier-1 Router.
- Select the **Configuration** tab and the **Ports** option.
- Verify that the router has a single linked port connecting the Tier-1 Router to the Tier-0 Router.

###<a id='s6'></a> Step 6: Configure a Small Load Balancer

Create a new small-size Load Balancer and attach it to the Tier1 router previously created.

<p class="note"><strong>Note</strong>: The small-size VM is suitable for the NSX Management Cluster load balancer. Make sure you have enough Edge Cluster resources to provision a small load balancer.</p>

- Select **Load Balancers**.
- Click **Add**.
- Enter a **Name** for the load balancer.
- Select the **SMALL** size load balancer.
- Click OK.

###<a id='s7'></a> Step 7: Attach the Load Balancer to the Router

Attach the load balancer to the Tier-1 Router previously created.

- Select the load balancer you just provisioned.
- Select the **Overview** tab.
- Select **Attachment** > **Edit**.
- **Tier-1 Logical Router**: Enter the name of the Tier-1 Router you configured, for example `T1-NSX-T-EXTERNAL-LB`.
- Click **OK**.

###<a id='s8'></a> Step 8: Configure a Virtual Server

Add and configure a virtual server for the load balancer.

- Select **Load Balancers** > **Virtual Servers**.
- Click **Add**.

Configure **General Properties** for the virtual server:

- **Name**: VS-NSX-T-EXTERNAL-LB
- **Application Types**: Layer 4 TCP
- **Application Profile**: default-tcp-lb-app-profile
- **Access Log**: Disabled
- Click **Next**

Configure **Virtual Server Identifiers** for the virtual server:

- **IP Address**: Enter an IP address from the floating pool, such as `10.40.14.250`.
- **Port**: 443
- Click **Next**.

Configure **Virtual Server Pool** for the virtual server:

- Click **Create a New Server Pool**.

Configure **General Properties** for the server pool:

- **Name**: For example `NSX-T-MGRS-SRV-POOL`
- **Load Balancing Algorithm**: ROUND_ROBIN
- Click **Next**

Configure **SNAT Translation** for the server pool:

- **Translation Mode**: IP List
- **IP address**: Enter the Virtual Switch IP (VIP) address here, for example `10.40.14.250`.
- Click **Next**.

Configure **Pool Members** for the server pool:

- **Membership Type**: Static.
- **Static Membership**: Add all 3 NSX Managers as members by entering the node name, IP address, and port (443) for each node.
- Click **Next**.

Configure **Health Monitors**:

- We will create the Health Monitors separately.
- Click **Finish**.

Back at the Server Pool screen, click **Next**.

Configure **Load Balancing Profiles** for the load balancer:

- **Persistence Profile** > **Source IP**: Select **default-source-ip-lb-persistence-profile**
- Click **Finish**.

<p class="note"><strong>Note:</strong> If a proxy is used between the NSX Management Cluster and the PKS Management Plane, do not configure a persistence profile.</p>

###<a id='s9'></a> Step 9: Attach the Virtual Server to the Load Balancer

Attach the virtual switch to the NSX-T load balancer.

- In the **Load Balancing** panel, select the Virtual Server you created.
- Select the **Load Balancers** tab.
- Click **Attach**.
- **Load Balancer**: Select the load balancer to attach, such as `NSX-T-EXTERNAL-LB`.
- Click **OK**.

###<a id='s10'></a> Step 10: Verify the Load Balancer.

Once the load balancer is configured, verify it by doing the following:

- Ping the NSX-T load balancer VIP address from your local machine.
- Access the NSX-T Manager interface using the load balancer VIP address, for example `https://10.40.14.250`.

<p class="note"><strong>Note:</strong> The URL redirects to the same NSX-T Manager. Persistence is done on the source IP based on the persistence profile you selected.</p>

###<a id='s11'></a> Step 11: Create an Active Health Monitor (HM)

Create a new Active Health Monitor (HM) for NSX Management Cluster members using the NSX-T Health Check protocol.

- Select **Load Balancers** > **Server Pools**.
- Select the server pool you created (for example, **NSX-T-MGRS-SRV-POOL**). 
- Select the **Overview** tab
- Click **Health Monitor** > **Edit**
- Click **Create a new active monitor**

Configure **Monitor Properties**:

- **Name**: `NSX-T-Mgr-Health-Monitor`
- **Health Check Protocol**: `LbHttpsMonitor`
- **Monitoring Port**: `443`

Configure **Health Check Parameters**. 

Configure the new Active HM with specific HTTP request fields as follows:

- **SSL Protocols**: Select the **TLS_v1** and **TLS_v2** protocols.
- **SSL Ciphers**: Select **Balanced** (recommended)

Configure the **HTTP Request Configuration** settings for the health monitor:

- **HTTP Method**: `GET`
- **HTTP Request URL**: `/api/v1/reverse-proxy/node/health`
- **HTTP Request Version**: `HTTP_VERSION_1_1`

Configure the **HTTP Request Headers** for the health monitor:

- **Authorization**: `Basic YWRtaW46Vk13YXJlMSE=`, which is the base64-encoded value of the NSX-T administrator credentials 
- **Content-Type**: `application/json`
- **Accept**: `application/json`

<p class="note"><strong>Note:</strong> In the example, <em>YWRtaW46Vk13YXJlMSE=</em> is the base64-encoded value of the NSX-T administrator credentials, expressed in the form <em>admin-user:password</em>. You can use the free online service <a href="https://www.base64encode.org/">www.base64encode.org</a> to base64 encode your NSX-T administrator credentials.</p>

Configure the **HTTP Response Configuration** for the health monitor:

- **HTTP Response Code**: `200`
- Click **Finish**.

At the Health Monitors screen, specify the Active Health Monitor you just created:

- **Active Health Monitor**: Enter a name for the health monitor, such as **NSX-T-Mgr-Health-Monitor**.
- Click **Finish**.

###<a id='s12'></a> Step 12: Create SNAT Rule

If your <%= vars.product_short %> deployment uses NAT mode, make sure Health Monitoring traffic is correctly SNAT-translated when leaving the NSX-T topology. Add a specific SNAT rule that intercepts HM traffic generated by the load balancer and translates this to a globally-routable IP Address allocated using the same principle of the load balancer VIP. The following screenshot illustrates an example of SNAT rule added to the Tier0 Router to enable HM SNAT translation. In the example, `100.64.128.0/31` is the subnet for the Load Balancer Tier-1 uplink interface.

To do this you need to retrieve the IP of the T1 uplink (Tier-1 Router that connected the NSX-T LB instance). In the example below, the T1 uplink IP is `100.64.112.37/31`.

Create the following SNAT rule on the Tier-0 Router:

- **Priority**: `2000`
- **Action**: `SNAT`
- **Source IP**: `100.64.112.36/31`, for example
- **Destination IP**: `10.40.206.0/25`, for example
- **Translated IP**: `10.40.14.251`, for example
- Click **Save**

- Verify configuration of the SNAT rule and server pool health:

###<a id='s13'></a> Step 13: Verify that NSX Manager Traffic Is Load Balanced

Verify the load balancer and that traffic is load balanced.

- Confirm that the status of the Logical Switch for the load balancer is Up.
- Confirm that the status of the Virtual Server for the load balancer is Up.
- Confirm that the status of the Server Pool is Up.
- Open an HTTPS session using multiple browser clients and confirm that traffic is load-balanced across different NSX-T Managers:

You can use the NSX API to validate that secure HTTP requests against the new VIP address are associated with the load balancer's Virtual Server. Relying on the SuperUser Principal Identity created as part of PKS provisioning steps, you can cURL the NSX Management Cluster using the standard HA-VIP address or the newly-provisioned virtual server VIP. For example:

Before load balancer provisioning is completed:

```
curl -k -X GET "https://192.168.6.210/api/v1/trust-management/principal-identities" --cert $(pwd)/pks-nsx-t-superuser.crt --key $(pwd)/pks-nsx-t-superuser.key
```

After load balancer provisioning is completed:

```
curl -k -X GET "https://91.0.0.1/api/v1/trust-management/principal-identities" --cert $(pwd)/pks-nsx-t-superuser.crt --key $(pwd)/pks-nsx-t-superuser.key
```

Key behavioral differences among the two API calls is the fact that the call toward the Virtual Server VIP will effectively Load Balance requests among the NSX-T Server Pool members. On the other hand, the call made toward the HA VIP address would ALWAYS select the same member (the Active Member) of the NSX Management Cluster.

Residual configuration step would be to change PKS Tile configuration for NSX-Manager IP Address to use the newly-provisioned Virtual IP Address. This configuration will enable any component internal to PKS (NCP, NSX OSB Proxy, BOSH CPI, etc.) to use the new Load Balancer functionality.
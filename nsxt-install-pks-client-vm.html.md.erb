---
title: Create PKS Client
owner: PKS-NSXT
---

<strong><%= modified_date %></strong>

This section provides instructions for creating the PKS Client VM and installing various command-line tools for interacting with the Enterprise PKS Platform. These tools are used by platform operators responsible for managing the PKS infrastructure and developers who are consumers of the Kubernetes cluster services.

##<a id='create-vm'></a> Create the VM

Create an Ubuntu VM to be used as the `pks-client`.

Alternatively, you can use the Ops Manager VM. For instructions, see the following knowledgebase article: [Pivotal](https://community.pivotal.io/s/article/generate-an-ssh-key-pair-for-installing-ops-manager-v2-6-on-vsphere) or [VMware](https://kb.vmware.com/s/article/71143).

1. Log in to the vSphere client for your environment.

1. Create new Ubuntu VM named `pks-cli-vm`.

  - Downoad an Ubuntu ISO.
  - Upload this ISO to a datastore in vSphere (such as vSAN).
  - Create a new VM in vSphere and complete the wizard.
  - Select the PKS-MGMT network.
  - Select Datastore ISO and browse to and select the Ubuntu ISO.
  - You have to install the OS. To do this, download an Ubuntu ISO, attach it as a CD to the VM and boot it.
  - Get the IP address for the `pks-cli-vm` from NSX Manager > Inventory > IP Pools tab in the ip-pool-snat NAT pool. For example, the routable IP address to my CLI-VM: 10.40.14.6.

1. Log in to the Ubuntu cli-vm host.

- Ubuntu
- {standard-password}

1. Upgrade and/or update the software: `sudo apt-get upgrade` | `sudo apt-get update`.

1. Create directory: `mkdir pks`.

##<a id='cli-install'></a> Install the PKS and Kubectl CLIs

1. Log in to the Pivotal network <https://network.pivotal.io/products/pivotal-container-service/> and download the following `pks-software`:

  - pks-linux-amd64-VERSION
  - kubectl-linux-amd64-VERSION

1. Copy CLIs to Ops Manager using SCP (PSCP in this case):

    pscp c:\temp\pks-linux-amd64-VERSION ubuntu@IP-ADDRESS:/home/ubuntu/pks
    pscp c:\temp\kubectl-linux-amd64-VERSION ubuntu@IP-ADDRESS:/home/ubuntu/pks

  For example:

    pscp c:\temp\pks\pks-linux-amd64-1.5.0-build.245 ubuntu@10.197.79.152:/home/ubuntu/pks
    pscp c:\temp\pks\kubectl-linux-amd64-1.14.1 ubuntu@10.197.79.152:/home/ubuntu/pks

1. Make the CLIs executable:

    chmod +x pks-linux-amd64-VERSION
    chmod +x kubectl-linux-amd64-VERSION

  For example:

    chmod +x pks-linux-amd64-1.5.0-build.245
    chmod +x kubectl-linux-amd64-1.14.1

1. Rename and move the files to the PATH:

    sudo mv pks-linux-amd64-1.5.0-build.245 /usr/local/bin/pks
    sudo mv kubectl-linux-amd64-1.13.5 /usr/local/bin/kubectl

1. Verify CLI installation:

	`pks --version`
	`kubectl version`

1. View CLI help.

	`pks --help`
	`kubectl --help`

##<a id='uaac-install'></a> Install UAAC

The UAA CLI (UAAC) is a command line interface for the Cloud Foundry User Account and Authentication (UAA) identity server. UACC is used to manage user accounts and authorization for the PKS platform. For more information, see https://github.com/cloudfoundry/cf-uaac.

1. Install UAAC:

    cd /usr/local/bin
    sudo apt -y install ruby ruby-dev gcc build-essential g++
    sudo gem install cf-uaac

  To verify installation:

    uaac -v

##<a id='om-install'></a> Install OM CLI

The Ops Manager CLI is used to deploy products to Pivotal Ops Manager. 

Repository: https://github.com/pivotal-cf/om
Documentation: https://docs.pivotal.io/pivotalcf/2-1/customizing/ops-man.html

To install the Ops Manager CLI, run these commands:

```
cd /usr/local/bin
sudo wget https://github.com/pivotal-cf/om/releases/download/0.38.0/om-linux
sudo chmod +x om-linux
sudo mv om-linux /usr/local/bin/om
```

To verify installation:

```
om -v
```

Expected result:

`0.38.0`

##<a id='bosh-install'></a> Install BOSH CLI

Bosh is used to manage PKS deployments and provides information about the VMs using its Cloud Provider Interface (CPI) which is vSphere in this case.

Documentation: https://bosh.io/docs/ and https://docs.pivotal.io/pivotalcf/2-2/customizing/deploy-bosh-om.html

Go here to find out Bosh CLI versions: https://s3.amazonaws.com/bosh-cli-artifacts/

To install the bosh cli, run the following commmands: 

```
sudo wget https://s3.amazonaws.com/bosh-cli-artifacts/bosh-cli-2.0.48-linux-amd64
sudo chmod +x bosh-cli-2.0.48-linux-amd64
sudo mv bosh-cli-2.0.48-linux-amd64 /usr/local/bin/bosh
```

Verify:

```
bosh -v
```

##<a id='nsxc-install'></a> Install NSX CLI

The NSX CLI is used to clean NSX-T objects after a Kubernetes cluster has been deleted.

```
sudo apt -y install git httpie jq
sudo wget https://storage.googleapis.com/pks-releases/nsx-helper-pkg.tar.gz
sudo tar -xvzf nsx-helper-pkg.tar.gz
sudo chmod +x nsx-cli.sh
```

To verify:

nsx-cli.sh

Result:

```
ubuntu@ubuntu:/usr/local/bin$ nsx-cli.sh
bash /usr/local/bin/nsx-cli.sh [category] [method] [parameters...]
ipam
  allocate
  release [IP_ADDRESS]
nat
  create-rule [CLUSTER_UUID] [MASTER_IP] [FLOATING_IP]
  delete-rule [CLUSTER_UUID]
cleanup [CLUSTER_UUID] [DRY_RUN]
```

##<a id='users-add'></a> Add PKS Users to UAA 

1. Add PKS API hostname and IP address to a DNS server.

  Get the PKS API Hostname from the PKS tile > PKS API module.

  Get the IP address from vSphere.

  Add the information to the /etc/hosts file, for example:

    vi /etc/hosts

    127.0.0.1 localhost
    ...
    10.0.0.7 api.pks.vsphere.local

1. Target PKS using UAAC:

    uaac target https://PKS-HOSTNAME:8443 --skip-ssl-validation

1. Fetch the UAA admin token.

  Retrieve the token from Ops Manager: PKS Tile > Credentials > Pks Uaa Management Admin Client

  Click on Link to Credential, copy the secret value.

  Run the following command:

    uaac token client get admin -s SECRET

  For example:

    uaac target https://api.comet.pks.local:8443 --skip-ssl-validation
    Unknown key: Max-Age = 86400

    Target: https://api.comet.pks.local:8443

    uaac token client get admin -s syMZA-FUlvdEXgXrgI-SDmkUVuP5rlzA
    Unknown key: Max-Age = 86400

    Successfully fetched token via client credentials grant.
    Target: https://api.comet.pks.local:8443
    Context: admin, from client admin

1. Add two users: PKS Admin and PKS Manage.

    uaac user add pks-admin --emails pks-admin@example.com -p PASSWORD
    user account successfully added
    
    uaac user add k8s-admin --emails k8s-admin@example.com -p PASSWORD
    user account successfully added
    
    uaac member add pks.clusters.admin pks-admin
    success
    uaac member add pks.clusters.manage k8s-admin
    success

##<a id='pks-login'></a> Log In to PKS 

1. Login as pks-admin.  

    pks login -a "api.pks.vsphere.local" -u "pks-admin" -p "VMware1!" -k

    API Endpoint: api.pks.vsphere.local
    User: pks-admin
    Login successful.

1. Log in a k8s-admin.

    pks login -a "api.pks.vsphere.local" -u "k8s-admin" -p "VMware1!" -k

    API Endpoint: api.comet.pks.local
    User: k8s-admin
    Login successful.

##<a id='pks-cluster'></a> Create Test Cluster

Run the following command:

	pks create-cluster CLUSTER_NAME -e CLUSTER_HOSTNAME -p PLAN_NAME

For example:

	pks create-cluster hello-world -e hello-world-cluster -p small

Run the following command to view:

	pks clusters

	pks cluster CLUSTER_NAME 

##<a id='bosh-verify'></a> Use BOSH to Manage PKS 

1. Create a BOSH alias.

  Go to BOSH Director tile > Credentials > Bosh Commandline Credentials.

  Copy the string `bosh_commandline_credentials` and create a Bash alias. For example:

    alias bosh="BOSH_CLIENT=ops_manager BOSH_CLIENT_SECRET=secret-from-bosh_commandline_credentials BOSH_CA_CERT=/var/tempest/workspaces/default/root_ca_certificate BOSH_ENVIRONMENT=ip-from-bosh_commandline_credentials bosh "

  Or, create `bosh_evn.sh` and run the command `source bosh_env.sh`.

    export BOSH_CLIENT=ops_manager
    export BOSH_CLIENT_SECRET=SECRET
    export BOSH_CA_CERT=/var/tempest/workspaces/default/root_ca_certificate
    export BOSH_ENVIRONMENT=IP-ADDRESS-OF-BOSH
    alias om=om-linux

1. Get bosh deployments.

    bosh deployments

1. Get bosh VMs.

    bosh vms

1. Get bosh components.

    bosh releases

1. SSH to a Kubernetes node.

    bosh ssh -d 'service-instance_9dd62330-fcea-469f-a50d-27c746622662' master/b1f99643-6905-4427-a01c-c26fc9101d40

1. Delete a Kubernetes cluster.

  This may be needed if a Kubernetes cluster fails to deploy, and you can't delete it using the PKS CLI.

    bosh -e pks delete-deployment -d service-instance_3bd9069f-aafd-4525-a145-f3e48570c055

##<a id='k8s-creds'></a> Connect to Kubernetes Cluster 

1. Retrieve credentials for the Kubernetes cluster.

    pks get-credentials CLUSTER-NAME

1. Verify that the /.kube/config file is created. 

  The following file is created:

    ls -l ~/.kube/config

1. Create a DNS record for the Kubernetes cluster and master node IP address.

  Run the following command to get the information:

    pks cluster CLUSTER-NAME

  Get the value for the "Kubernetes Master Host" and the value for the "Kubernetes Master IP(s)".

  Use these values to create a DNS record. For example:

    vi /etc/hosts

    10.197.100.130 tea-cluster

1. Connect to the Kubernetes cluster.

    kubectl config use-context CLUSTER-NAME

1. Get cluster nodes.

    kubectl get nodes

##<a id='hello-world'></a> Deploy a Hello World Cluster 

Complete the following Docker and Kubernetes tasks to deploy a test cluster.

###<a id='docker'></a> Part I: Create, Run, Push Docker Image

#### Step 1: Download the gowebapp code under your home directory and untar the file

```
wget https://s3.eu-central-1.amazonaws.com/heptio-edu-static/how/gowebapp.tar.gz

tar -zxvf gowebapp.tar.gz
```

#### Step 2: Create Dockerfile for your frontend application

```
cd $HOME/gowebapp/gowebapp
```

Create a file named Dockerfile in this directory for the frontend Go application. Use vi or any preferred text editor. Populate it with the following code snippet.

```
LABEL maintainer "education@heptio.com"
LABEL version "1.0"

COPY ./code /opt/gowebapp
COPY ./config /opt/gowebapp/config

EXPOSE 80

WORKDIR /opt/gowebapp/
ENTRYPOINT ["/opt/gowebapp/gowebapp"]
```

#### Step 3: Build gowebapp Docker image locally

Build the gowebapp image locally. Make sure to include “.“ at the end. Make sure the build runs to completion without errors. You should get a success message.

`cd $HOME/gowebapp/gowebapp`

`docker build -t gowebapp:v1 .`

#### Step 4: Create Dockerfile for your backend application

```
cd $HOME/gowebapp/gowebapp-mysql
```

Create a file named Dockerfile in this directory for the backend MySQL database. Use vi or any preferred text editor. Populate it with the following code snippet.

```
FROM mysql:5.6

LABEL maintainer "education@heptio.com"
LABEL version="1.0"

COPY gowebapp.sql /docker-entrypoint-initdb.d/
```

#### Step 5: Build gowebapp-mysql Docker image locally

```
cd $HOME/gowebapp/gowebapp-mysql
```

Build the gowebapp-mysql image locally. Make sure to include “.“ at the end. Make sure the build runs to completion without errors. You should get a success message.

```
docker build -t gowebapp-mysql:v1 .
```

#### Step 5. Run and test Docker images locally

Before deploying to Kubernetes, let’s run and test the Docker images locally, to ensure that the frontend and backend containers run and integrate properly.

#### Step 6: Create Docker user-defined network

To facilitate cross-container communication, let’s first define a user-defined network in which to run the frontend and backend containers:

`docker network create gowebapp`

#### Step 7: Launch frontend and backend containers

Next, let’s launch a frontend and backend container using the Docker CLI.

```
docker run --net gowebapp --name gowebapp-mysql --hostname gowebapp-mysql -d -e MYSQL_ROOT_PASSWORD=heptio gowebapp-mysql:v1

sleep 20

docker run -p 9000:80 --net gowebapp -d --name gowebapp --hostname gowebapp gowebapp:v1
```

First, we launched the database container, as it will take a bit longer to startup, and the frontend container depends on it. Notice how we are injecting the database password into the MySQL configuration as an environment variable:

Next we paused for 20 seconds to give the database a chance to start and initialize.

Finally we launched a frontend container, mapping the container port 80 - where the web application is exposed - to port 9000 on the host machine:

#### Step 8: Test the application locally

Now that we’ve launched the application containers, let’s try to test the web application locally. You should be able to access the application at http://<EXTERNAL-IP>:9000.

To get the EXTERNAL-IP of your host, run the command lab-info in your lab terminal

Create an account and login. Write something on your Notepad and save it. This will verify that the application is working and properly integrates with the backend database container.

#### Step 9: Inspect the MySQL database

Let’s connect to the backend MySQL database container and run some queries to ensure that application persistence is working properly:

docker exec -it gowebapp-mysql mysql -u root -pheptio gowebapp
Once connected, run some simple SQL commands to inspect the database tables and persistence:

```
#Simple SQL to navigate
SHOW DATABASES;
USE gowebapp;
SHOW TABLES;
SELECT * FROM <table_name>;
exit;
```

#### Step 10: Cleanup application containers

When we’re finished testing, we can terminate and remove the currently running frontend and backend containers from our local machine:

`docker rm -f gowebapp gowebapp-mysql`

### Part IV. Create and push Docker images to Docker registry

#### Step 1: Tag images to target another registry

We are finished testing our images. We now need to push our images to an image registry so our Kubernetes cluster can access them. First, we need to tag our Docker images to use the registry in your lab environment:

`docker tag gowebapp:v1 localhost:5000/gowebapp:v1`

`docker tag gowebapp-mysql:v1 localhost:5000/gowebapp-mysql:v1`

#### Step 2: publish images to the registry

```
docker push localhost:5000/gowebapp:v1
docker push localhost:5000/gowebapp-mysql:v1
```

#### Step 3: Verify repository images

To verify push of images to the registry:

```
curl localhost:5000/v2/_catalog
```

Lab 01 Conclusion

Congratulations! By containerizing your application components, you have taken the first important step toward deploying your application to Kubernetes.


-----

## Lab 02: Deploy Applications Using Kubernetes

### Getting Started with kubectl

kubectl is the command line interface for interacting with a Kubernetes cluster. Let’s explore some of it’s features.

#### Step 1: introduction to kubectl

By executing kubectl you will get a list of options you can utilize kubectl for. kubectl allows you to control Kubernetes cluster manager.

kubectl

#### Step 2: use kubectl to understand an object

Use explain to get documentation of various resources. For instance pods, nodes, services, etc.

kubectl explain pods

#### Step 3: get more information on an object

Shortcut to object names:

kubectl describe

#### Step 4: autocomplete

Use TAB to autocomplete:

kubectl describe <TAB> <TAB>

### Create Service Object for MySQL

#### Step 1: define a Kubernetes Service object for the backend MySQL database

cd $HOME/gowebapp/gowebapp-mysql

Create a file named gowebapp-mysql-service.yaml in this directory. Use vi or any preferred text editor. Populate it with the following code snippet.

gowebapp-mysql-service.yaml

```
apiVersion: v1
kind: Service
metadata: 
  name: gowebapp-mysql
  labels:
    run: gowebapp-mysql
    tier: backend
spec:
  type: ClusterIP
  ports:
  - port: 3306
    targetPort: 3306
  selector:
    run: gowebapp-mysql
    tier: backend
```

#### Step 2: create a Service defined above

Use kubectl to create the service defined above

kubectl apply -f gowebapp-mysql-service.yaml

#### Step 3: test to make sure the Service was created

kubectl get service -l "run=gowebapp-mysql"

### Create Deployment Object for MySQL

#### Step 1: define a Kubernetes Deployment object for the backend MySQL database

`cd $HOME/gowebapp/gowebapp-mysql`

Create a file named gowebapp-mysql-deployment.yaml in this directory. Use vi or any preferred text editor. Populate it with the following code snippet.

gowebapp-mysql-deployment.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: gowebapp-mysql
  labels:
    run: gowebapp-mysql
    tier: backend
spec:
  replicas: 1
  strategy: 
    type: Recreate
  selector:
    matchLabels:
      run: gowebapp-mysql
      tier: backend
  template:
    metadata:
      labels:
        run: gowebapp-mysql
        tier: backend
    spec:
      containers:
      - name: gowebapp-mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: heptio
        image: localhost:5000/gowebapp-mysql:v1
        ports:
        - containerPort: 3306
```

#### Step 2: create the Deployment defined above

Use kubectl to create the service defined above

kubectl apply -f gowebapp-mysql-deployment.yaml

#### Step 3: test to make sure the Deployment was created

kubectl get deployment -l "run=gowebapp-mysql"

### Create Service Object for frontend application: gowebapp

#### Step 1: define a Kubernetes Service object for the frontend gowebapp

`cd $HOME/gowebapp/gowebapp`

Create a file named gowebapp-service.yaml in this directory. Use vi or any preferred text editor. Populate it with the following code snippet.

gowebapp-service.yaml

```
apiVersion: v1
kind: Service
metadata: 
  name: gowebapp
  labels:
    run: gowebapp
    tier: frontend
spec:
  type: NodePort
  ports:
  - port: 80
  selector:
    run: gowebapp
    tier: frontend
```

#### Step 2: create a Service defined above

Use kubectl to create the service defined above

`kubectl apply -f gowebapp-service.yaml`

#### Step 3: test to make sure the Service was created

`kubectl get service -l "run=gowebapp"`

### Create Deployment Object for gowebapp

#### Step 1: define a Kubernetes Deployment object for the frontend gowebapp

`cd $HOME/gowebapp/gowebapp`

Create a file named gowebapp-deployment.yaml in this directory. Use vi or any preferred text editor. Populate it with the following code snippet.

gowebapp-deployment.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: gowebapp
  labels:
    run: gowebapp
    tier: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      run: gowebapp
      tier: frontend
  template:
    metadata:
      labels:
        run: gowebapp
        tier: frontend
    spec:
      containers:
      - name: gowebapp
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: heptio
        image: localhost:5000/gowebapp:v1       
        ports:
        - containerPort: 80
```

#### Step 2: create the Deployment defined above

Use kubectl to create the service defined above

`kubectl apply -f gowebapp-deployment.yaml`

#### Step 3: test to make sure the Deployment was created

`kubectl get deployment -l "run=gowebapp"`


### Test Your Application

#### Step 1: Access gowebapp through the NodePort service

Access your application at the NodePort Service endpoint: http://<EXTERNAL-IP>:<NodePort>.

Note

To get the EXTERNAL-IP of your host, run the command lab-info in your lab terminal
Note

To get the NodePort for your service, run the command kubectl get svc gowebapp in your lab terminal

*** EXAMPLE OUTPUT ****

$ kubectl get svc gowebapp

NAME       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
gowebapp   NodePort   10.107.169.105   <none>        80:30500/TCP   2m

In this example, the NodePort is 30500

You can now sign up for an account, login and use the Notepad.

Note: Your browser may cache an old instance of the gowebapp application from previous labs. When the webpage loads, look at the top right. If you see ‘Logout’, click it. You can then proceed with creating a new test account.
Lab 02 Conclusion

Congratulations! You have successfully deployed your applications using Kubernetes!




##<a id='home'></a> NSX-T Installation Instructions Home

<a href="./nsxt-install-config-steps.html">Installing and Configuring NSX-T for <%= vars.product_short %></a>.

---
title: Release Notes
owner: TKGI
topictype: releasenotes
---

This topic contains release notes for <%= vars.product_short %> (<%= vars.k8s_runtime_abbr %>) v1.11.

<p class="note warning"><strong>Warning:</strong> Before installing or upgrading to <%= vars.product_short %> v1.11,
review the <a href="#1-11-0-breaking-changes">Breaking Changes</a> below.
</p>

## <a id="1-11-0"></a><%= vars.k8s_runtime_abbr %> v1.11.0

**Release Date**: May TBD, 2021

### <a id='1-11-0-snapshot'></a><a id='product-snapshot'></a>Product Snapshot

<table class="nice">
    <tr>
        <th>Release</th>
        <th>Details</th>
    </tr>
    <tr>
        <td>Version</td>
        <td>v1.11.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>May TBD, 2021</td>
    </tr>
    <tr>
        <th>Component</th>
        <th>Version</th>
    </tr>
    <tr>
        <td>Kubernetes</td>
        <td>v1.20.5</td>
    </tr>
    <tr>
        <td>CoreDNS</td>
        <td>v1.7.0+vmware.8</td>
    </tr>
    <tr>
        <td>Docker</td>
        <td>
            Linux: v19.03.14<br>
            Windows: v19.03.14
        </td>
    </tr>
    <tr>
        <td>etcd</td>
        <td>v3.4.13</td>
    </tr>
    <tr>
        <td>Metrics Server</td>
        <td>v0.3.6</td>
    </tr>
    <tr>
        <td>NCP</td>
        <td><strong>PENDING: v3.1.2</strong></td>
    </tr>
    <tr>
        <td>Percona XtraDB Cluster (PXC)</td>
        <td>v0.33.0</td>
    </tr>
    <tr>
        <td>UAA</td>
        <td>v74.5.22</td>
    </tr>
    <tr>
        <th>Compatibilities</th>
        <th>Versions</th>
    </tr>
    <tr>
        <td>VMware Cloud Foundation (VCF)</td>
        <td><strong>PENDING: v4.3 L1</strong></td>
    </tr>
    <tr>
        <td>Ops Manager</td>
        <td>Ops Manager 2.10.8 or later or 2.11.0 or later.</td>
    </tr>
    <tr>
        <td>Xenial stemcells</td>
        <td>See <a href="https://network.pivotal.io/products/pivotal-container-service/"><%= vars.product_network %>.</a></td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>v2019.34+</td>
    </tr>
    <tr>
        <td>vSphere</td>
        <td rowspan=2>See <a href="https://interopmatrix.vmware.com/#/Interoperability?isHideGenSupported=true&isHideTechSupported=true&isHideCompatible=false&isHideIncompatible=false&isHideNTCompatible=false&isHideNotSupported=true&isCollection=false&col=644&row=0,">VMware Product Interoperability Matrices.</a></td></td>
    </tr>
    <tr>
        <td>NSX-T</td>
    </tr>
    <tr>
        <td>CNS for vSphere</td>
        <td>v2.2.0</td>
    </tr>
    <tr>
        <td>Harbor</td>
        <td>v2.2.x</td>
    </tr>
    <tr>
        <td>Velero</td>
        <td><a href="https://my.vmware.com/en/group/vmware/downloads/details?downloadGroup=VELERO-142&productId=1113&rPId=58884">v1.4.2</a></td>
    </tr>
</table>

### <a id='1-11-0-upgrade'></a>Upgrade Path

The supported upgrade paths to <%= vars.k8s_runtime_abbr %> v1.11.0 are from <%= vars.product_short %> v1.10.0 and later.

### <a id="1-11-0-features"></a>Features

This section describes new features and changes in <%= vars.product_short %> v1.11.0.  

* **[Bug Fix]** Fixes [The TKGI CLI Resize and Update Cluster Commands Remove the Network Profile CNI Configuration from a Cluster]
(https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-do-notuse-resize-cluster). 
* **[Bug Fix]** Fixes [<%= vars.k8s_runtime_abbr %> CLI `get-credentials` Returns an â€œod-broker" Error](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-od-broker).  
* **[Bug Fix]** Fixes [Windows Pods Are Unable to Complete the Drain Process during a TKGI Upgrade or Update](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-windows-drain-error).  
* **[Bug Fix]** Fixes [Cluster Resize Fails for a Cluster Configured With a Network Profile Containing the `failover_mode` Parameter](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-failover-mode).  
* **[Bug Fix]** Fixes [Certain Linux Nodes Are Unable to Complete the Drain Process during a TKGI Upgrade](https://docs.pivotal.io/tkgi/1-10/release-notes.html#bosh-stop-doesnt-drain).  
* **[Bug Fix]** Fixes [Your Cluster Returns the Error 'PodCIDR is Empty for Node'](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-stale-ip-problem).  
* **[Bug Fix]** Fixes [The TKGI API Does Not Import the Current TKGI CA Certificates](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-import-single-cert).  
<%#
* **[Bug Fix]** Fixes [Database Cluster Stops After a DB Instance Is Stopped](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-db-down-after-vm-stopped).  
>#%>
* **[Security Fix]** Passes additional CIS Kubernetes Benchmarks. See [<%= vars.k8s_runtime_abbr %> Cluster Benchmarks](./security.html#benchmarks) for details.  
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 supports uppercase characters in <%= vars.k8s_runtime_abbr %> API command lines.
For more information, see [FQDNs in <%= vars.k8s_runtime_abbr %> API Commands Cannot Contain Uppercase Letters](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-uppercase-bosh-dns).  
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 supports NSX-T v3.1 environments where the Transport Zone and Edge Transport Node switch names are not identical. 
For more information, see [On NSX-T v3.1 the Transport Zone and Edge Transport Node Switch Names Must be Identical](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-node-names-match).  
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 supports using Network Profiles to add additional DNS server IPs to existing clusters. 
For information about using a Network Profile to update a cluster with additional DNS server IPs, see [Confirm the Network Profile Property Supports Updates](network-profiles-define.html#supports-updates) 
in _Creating and Deleting Network Profiles (NSX-T Only)_.  
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 supports the Kubernetes Host Port feature. 
The Kubernetes Host Port feature allows you to expose an application to be externally accessible through a single port. 
Host Port support requires a Host Port-compatible CNI. Host Port support does not require privileged mode. 
For more information, see [Creating and Deleting Network Profiles (NSX-T Only)](network-profiles-define.html).  
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 supports enabling the NCP 3.1 Kubernetes NodeLocal DNSCache feature. 
NodeLocal DNSCache improves cluster DNS performance by running a DNS caching agent on cluster nodes. The agent runs on the nodes as a DaemonSet. 
For more information, see [Creating and Deleting Network Profiles (NSX-T Only)](network-profiles-define.html).  
* **[Enhancement]** CoreDNS in <%= vars.k8s_runtime_abbr %> v1.11.0 and later is highly available. 
In multi-node Pods, CoreDNS functions are now automatically distributed across Pods across your designated AZs. 
CoreDNS in an existing <%= vars.k8s_runtime_abbr %> multi-node Pod is automatically reconfigured for high availability when the cluster is upgraded 
to <%= vars.k8s_runtime_abbr %> v1.11.   
* **[Enhancement]** <%= vars.k8s_runtime_abbr %> v1.11.0 returns an error if a network profile configuration changes a parameter that does not support 
modification.  

#### <a id="1-11-0-automatic-cns"></a>Automatic vSphere CSI Driver Integration

<%= vars.k8s_runtime_abbr %> v1.11.0 supports automatic vSphere CSI Driver installation on vSphere and vSphere with NSX&#8209;T. 
Automatic vSphere CSI Driver integration provides CNS as a process, allowing CNS volumes to be deployed to clusters without requiring access to IaaS credentials. 
For more information, see [Deploying Cloud Native Storage (CNS) on vSphere](vsphere-cns.html).  


#### <a id="1-11-0-vm-extensions"></a>Support for BOSH VM&nbsp;Extensions

<%= vars.k8s_runtime_abbr %> v1.11.0 supports configuring new and existing Kubernetes clusters using BOSH VM&nbsp;Extensions. 
You can modify <%= vars.k8s_runtime_abbr %>-provisioned Kubernetes clusters using BOSH VM Extensions on AWS, Azure and vSphere. 
For more information, see [Using BOSH VM&nbsp;Extensions](bosh-vm-extensions.html).  


#### <a id="1-11-0-component-updates"></a>Component Updates

The following components have been updated:

<%#
* Bumps Kubernetes to v1.19.4.
#%>

### <a id="1-11-0-breaking-changes"></a> Breaking Changes

TKGI v1.11.0 has the following breaking changes.

* The built-in Clair container image scanner is deprecated in favor of Trivy. If you have enabled Clair, do one of the following before upgrading to Harbor tile v2.2.1:
    * In the Harbor tile, select the Trivy scanner as the default in "Interrogation Service" or
    * Install the Clair scanner outside the Harbor tile VM and configure the Clair scanner in "Interrogation Service" as the default scanner. For more information, see [Getting Started With Clair](https://github.com/quay/clair/blob/main/Documentation/howto/getting_started.md) in the Clair documentation.

###<a id='1-11-0-known-issues'></a>Known Issues

<%= vars.k8s_runtime_abbr %> v1.11.0 has the following known issues: 


#### <a id="1-11-0-nsxt-302-310"></a> Pods Stop After Upgrading From NSX-T v3.0.2 to v3.1.0 on vSphere 7.0 and 7.0.1

**Symptom**

Your TKGI-provisioned Pods stop after upgrading from NSX-T v3.0.2 to NSX-T v3.1.0 on vSphere 7.0 and 7.0.1.

**Explanation**

For information, see [Issue 2603550: Some VMs are vMotioned and lose network connectivity during UA nodes upgrade]
(https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/rn/VMware-NSX-T-Data-Center-311-Release-Notes.html#2603550) in the _VMware NSX-T Data Center 3.1.1 Release Notes_.

**Workaround**

To avoid the loss of network connectivity during UA node upgrade, ensure DRS is set to manual mode during your upgrade from NSX-T v3.0.2 to v3.1.0.

If you upgraded to NSX-T v3.1.0 with DRS in automation mode, run the following on the affected Pods' master VMs to restore Pod connectivity:

```

monit restart ncp

```

For more information on upgrading NSX-T v3.0.2 to NSX-T v3.1.0, see [Upgrade NSX-T Data Center to v3.0 or v3.1](upgrade-nsxt.html#upgrade-nsxt).  

#### <a id="1-11-0-azure-apply-changes"></a> Error: Could Not Execute "Apply-Changes" in Azure Environment

**Symptom**

After clicking **Apply Changes** on the <%= vars.k8s_runtime_abbr %> tile in an Azure environment, you experience
an error '_...could not execute "apply-changes"..._' with either of the following descriptions:

* _{"errors":{"base":["undefined method 'location' for nil:NilClass"]}}_
* _FailedError.new("Resource Groups in region '#{location}' do not support Availability Zones"))_

For example:

```
INFO | 2020-09-21 03:46:49 +0000 | Vessel::Workflows::Installer#run | Install product (apply changes)
2020/09/21 03:47:02 could not execute "apply-changes": installation failed to trigger: request failed: unexpected response from /api/v0/installations:
HTTP/1.1 500 Internal Server Error
Transfer-Encoding: chunked
Cache-Control: no-cache, no-store
Connection: keep-alive
Content-Type: application/json; charset=utf-8
Date: Mon, 21 Sep 2020 17:51:50 GMT
Expires: Fri, 01 Jan 1990 00:00:00 GMT
Pragma: no-cache
Referrer-Policy: strict-origin-when-cross-origin
Server: Ops Manager
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Download-Options: noopen
X-Frame-Options: SAMEORIGIN
X-Permitted-Cross-Domain-Policies: none
X-Request-Id: f5fc99c1-21a7-45c3-7f39
X-Runtime: 9.905591
X-Xss-Protection: 1; mode=block

44
{"errors":{"base":["undefined method `location' for nil:NilClass"]}}
0
```

**Explanation**

The Azure CPI endpoint used by Ops Manager has been changed and
your installed version of Ops Manager is not compatible with the new endpoint.

**Workaround**

Run the following Ops Manager CLI command:

```
om --skip-ssl-validation --username USERNAME --password PASSWORD --target https://OPSMAN-API curl --silent --path /api/v0/staged/director/verifiers/install_time/IaasConfigurationVerifier -x PUT -d '{ "enabled": false }'
```

Where:

* `USERNAME` is the account to use to run Ops Manager API commands.
* `PASSWORD` is the password for the account.
* `OPSMAN-API` is the IP address for the Ops Manager API


For more information, see [Error 'undefined method location' is received when running Apply Change on Azure]
(https://community.pivotal.io/s/article/undefined-method-location-when-running-Apply-Change-on-Azure?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="1-11-0-vrops-windows-clusters"></a> VMware vRealize Operations Does Not Support Windows Worker-Based Kubernetes Clusters

VMware vRealize Operations (vROPs) does not support Windows worker-based Kubernetes clusters and cannot be used to
manage <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.


#### <a id='1-11-0-wavefront-no-win'></a><%= vars.k8s_runtime_abbr %> Wavefront Requires Manual Installation for Windows Workers

To monitor Windows-based worker node clusters with a Wavefront collector and proxy, you must first install Wavefront on the clusters manually, using Helm.
For instructions, see the [Wavefront](windows-monitoring.html#wavefront) section of the _Monitoring Windows Worker Clusters and Nodes_ topic.

#### <a id='1-11-0-ping'></a>Pinging Windows Worker Kubernetes Clusters Does Not Work

<%= vars.k8s_runtime_abbr %>-provisioned Windows worker-based Kubernetes clusters inherit a Kubernetes limitation that prevents
outbound ICMP communication from workers.
As a result, pinging Windows workers does not work.

For information about this limitation, see [Limitations > Networking](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#networking-1)
in the _Windows in Kubernetes_ documentation.

#### <a id="1-11-0-windows-velero-limitations"></a> Velero Does Not Support Backing Up Stateful Windows Workloads

You can use Velero to backup stateless <%= vars.k8s_runtime_abbr %>-provisioned Windows workers.
Velero can back up stateless Windows workloads only, and cannot be used to backup stateful Windows applications.
For more information, see [Velero on Windows](https://velero.io/docs/v1.4/basic-install/#velero-on-windows) in
_Basic Install_ in the Velero documentation.

#### <a id="1-11-0-tmc-on-gcp"></a>Tanzu Mission Control Integration Not Supported on GCP

<%= vars.k8s_runtime_abbr %> on Google Cloud Platform (GCP) does not support
Tanzu Mission Control (TMC) integration, which is configured in
the **<%= vars.product_tile %>** tile > the **Tanzu Mission Control (Experimental)** pane.

If you intend to run <%= vars.k8s_runtime_abbr %> on GCP,
skip this pane when configuring the <%= vars.product_tile %> tile.

#### <a id='1-11-0-tmc-restic'></a>TMC Data Protection Feature Requires Privileged <%= vars.k8s_runtime_abbr %> Containers
TMC Data Protection feature supports privileged <%= vars.k8s_runtime_abbr %> containers only.
For more information, see [Plans](installing-vsphere.html#plans) in the _Installing TKGI_ topic for your IaaS.

#### <a id="1-11-0-profile-no-win-gmsa"></a>Windows Worker Kubernetes Clusters with Group Managed Service Account Do Not Support Compute Profiles 

Windows worker-based Kubernetes clusters integrated with group Managed Service Account (gMSA) cannot be managed using compute profiles.  

#### <a id="1-11-0-profile-no-win-flannel"></a> Windows Worker Kubernetes Clusters on Flannel Do Not Support Compute Profiles

On vSphere with NSX-T networking you can use compute profiles with both Linux and Windows worker&#8209;based Kubernetes clusters.
On vSphere with Flannel networking, you can apply compute profiles only to Linux clusters.


#### <a id="1-11-0-profile-resize-down"></a>TKGI CLI Does Not Prevent Reducing the Control Plane Node Count

TKGI CLI does not prevent accidentally reducing a cluster's control plane node count using a compute profile.

<p class="note warning"><strong>Warning:</strong>
    Reducing a cluster's control plane node count can destroy the cluster.
    Do not scale out or scale in existing master nodes by reconfiguring the TKGI tile or by using a compute profile.
    Reducing a cluster's number of control plane nodes may remove a master node and cause the cluster to become inactive.
</p>

#### <a id="1-11-0-profile-bosh-upgrade"></a>Compute Profile Dropped From Clusters During BOSH Upgrade

If a cluster created using a compute profile is upgraded using `tkgi upgrade-cluster`,
the cluster's compute profile will be dropped.

#### <a id="1-11-0-in-windows-notready-nodes"></a> Windows Cluster Nodes Not Deleted After VM Deleted

**Symptom**

After you delete a VM using the management console of your infrastructure provider, you notice a Windows worker node
that had been on that VM is now in a `notReady` state.

**Solution**

1. To identify the leftover node:

    ```
    kubectl get no -o wide
    ```
1. Locate nodes on the returned list that are in a `notReady` state and have the same IP address as another node in the list.
1. To manually delete a `notReady` node:

    ```
    kubectl delete node NODE-NAME
    ```
    Where `NODE-NAME` is the name of the node in the `notReady` state.

#### <a id="1-11-0-oidc-response-header"></a>502 Bad Gateway After OIDC Login

**Symptom**

You experience a "502 Bad Gateway" error from the NSX load balancer after you log in to OIDC.

**Explanation**

A large response header has exceeded your NSX-T load balancer maximum
response header size. The default maximum response header size is 10,240 characters and should
be resized to 50,000.

**Workaround**

If you experience this issue, manually reconfigure your NSX-T `request_header_size`
and `response_header_size` to 50,000 characters.
For information about configuring NSX-T default header sizes,
see [OIDC Response Header Overflow](https://community.pivotal.io/s/article/OIDC-Response-Header-overflow) in the Knowledge Base.


#### <a id='1-11-0-eight-cores-error'></a> NSX-T Pre-Check Errand Fails Due to Edge Node Configuration

**Symptom**

You have configured your NSX-T Edge Node VM as `medium` size,
and the NSX-T Pre-Check Errand fails with the following error:
"_ERROR: NSX-T Precheck failed due to Edge Node ... no of cpu cores is less than 8_".

**Explanation**

The NSX-T Pre-Check Errand is erroneously returning the "_cpu cores is less than 8_" error.

**Solution**

You can safely configure your NSX-T Edge Node VMs as `medium` size and ignore the error.

#### <a id='1-11-0-windows-proxy'></a> Difficulty Changing Proxy for Windows Workers

You must configure a global proxy in the <%= vars.product_tile %> tile > **Networking** pane before you create any Windows workers that use the proxy.

You cannot change the proxy configuration for Windows workers in an existing cluster.

#### <a id='1-11-0-http-proxy-password'></a> Character Limitations in HTTP Proxy Password

For vSphere with NSX-T, the HTTP Proxy password field does not support the following special characters: `&` or `;`.

#### <a id='1-11-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.

#### <a id='1-11-0-antrea-logging-interupt'></a> Unexplained Errors After Interrupting a Log Stream When Using Antrea Networking 

**Symptom**

While using Antrea networking, you observe unexplainable errors after you interrupt a log stream started using `kubectl logs -f POD-NAME`. 
The errors could include any of the following:  

* kubectl returns the error: "_Error from server (TooManyRequests): the server has received too many_".   
* `kube-apiserver` returns an http code `429`. 

**Explanation**

When using Antrea networking there is a chance that `konnectivity-agent` will become unstable after interrupting your kubectl log steam.

**Workaround**

To resolve the issue:  

1. Log in to the master VM:

    ```
    bosh -d DEPLOYMENT-NAME ssh master/0
    ````

1. Change to root:

    ```
    sudo -i
    ```

1. Restart `proxy-server`:

    ```
    monit restart proxy-server
    ```

1. Wait for `proxy-server` restart:

    ```
    monit summary
    ```


#### <a id='1-11-0-resizing-worker-nodes'></a> Ingress Controller Statefulset Fails to Start After Resizing Worker Nodes

**Symptom**  

Permissions are removed from your clusterâ€™s files and processes after resizing the persistent disk 
during a cluster upgrade. The ingress controller statefulset fails to start.

**Explanation**  

When resizing a persistent disk, Bosh migrates the data from the old disk to the new disk but 
does not copy the filesâ€™ extended attributes.

**Workaround**  

To resolve the problem, complete the steps in 
[Ingress controller statefulset fails to start after resize of worker nodes with permission denied]
(https://community.pivotal.io/s/article/5000e00001nCJxT1603094435795?language=en_US)
in the VMware Tanzu Knowledge Base.

#### <a id="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, on Azure the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.


#### <a id='1-11-0-uuid-length'></a> One Plan ID Longer than Other Plan IDs

**Symptom**

One of your plan IDs is one character longer than your other plan IDs.

**Explanation**

In <%= vars.k8s_runtime_abbr %>, each plan has a unique plan ID.
A plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens.
However, the **Plan 4** ID consists of 33 alphanumeric characters and 4 hyphens.

**Solution**

You can safely configure and use **Plan 4**.
The length of the **Plan 4** ID does not affect the functionality of **Plan 4** clusters.

If you require all plan IDs to have identical length, do not activate or use **Plan 4**.


#### <a id='1-11-0-whitespace-pksapi'></a> The <%= vars.control_plane %> FQDN Must Not Include Trailing Whitespace

**Symptom**

Your <%= vars.k8s_runtime_abbr %> logs include the following error:

```
'uaa'. Errors are:- Error filling in template 'uaa.yml.erb' (line 59: Client redirect-uri is invalid: uaa.clients.pks_cli.redirect-uri Client redirect-uri is invalid: uaa.clients.pks_cluster_client.redirect-uri)
```

**Explanation**

The <%= vars.control_plane %> fully-qualified domain name (FQDN) for your cluster contains leading or trailing whitespace.  

**Workaround**

Do not include whitespace in the <%= vars.k8s_runtime_abbr %> tile **API Hostname (FQDN)** field.  


#### <a id='1-11-0-db-down-after-vm-stopped'></a> Database Cluster Stops After a DB Instance Is Stopped 

**Symptom**

After you stop one instance in a multiple-instance database cluster, the cluster stops, 
or communication between the remaining databases times out, and the entire cluster becomes unreachable.

The following might be in your UAA log:

```
WSREP has not yet prepared node for application use
```

**Explanation** 

The database cluster is unable to recover automatically because a member is no longer available to reconcile quorum. 


## <a id="management-console-1-11-0"></a> <%= vars.k8s_runtime_abbr %> Management Console v1.11.0

**Release Date**: May TBD, 2021

### <a id="management-console-1.11.0-snapshot"></a>Product Snapshot

<p class="note"><strong>Note</strong>: <%= vars.product_short %> Management Console provides an opinionated
    installation of <%= vars.k8s_runtime_abbr %>. The supported versions may differ from or be more limited than
    what is generally supported by <%= vars.k8s_runtime_abbr %>.
</p>

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
      <td>Version</td>
      <td>v1.11.0</td>
    </tr>
    <tr>
      <td>Release date</td>
      <td>May TBD, 2021</td>
    </tr>
    <tr>
      <td>Installed Tanzu Kubernetes Grid Integrated Edition version</td>
      <td>v1.11.0</td>
    </tr>
    <tr>
      <td>Installed Ops Manager version</td>
      <td>2.10.8</td>
    </tr>
    <tr>
      <td>Installed Kubernetes version</td>
        <td>1.19.9</td>
    </tr>
    <tr>
      <td>Installed Harbor Registry version</td>
      <td>2.1.3</td>
    </tr>
        <tr>
        <td>Linux stemcell</td>
        <td>621.117</td>
    </tr>
    <tr>
        <td>Windows stemcells</td>
        <td>&gt;=2019.31</td>
    </tr>
</table>

<br>
### <a id='management-console-1-11-0-upgrade'></a>Upgrade Path

The supported upgrade path to <%= vars.product_short %> Management Console v1.11.0 is from
<%= vars.product_short %> v1.10.0 and later.

### <a id="management-console-1-11-0-features"></a>Features

<%= vars.product_short %> Management Console v1.11.0 updates include:  

* **[Bug Fix]** Fixes [Management Console UI Does Not Open If the Management Console Uses Custom Certificates](https://docs.pivotal.io/tkgi/1-10/release-notes.html#management-console-1.10.0-ui-customcerts).  
* **[Enhancement]** TKGI Management Console v1.11.0 can retrieve the transport zone of an NSX-T v3.1 edge node transport node. 
For more information, see [On NSX-T v3.1 the Transport Zone and Edge Transport Node Switch Names Must be Identical](https://docs.pivotal.io/tkgi/1-10/release-notes.html#1-10-0-node-names-match).  

### <a id="management-console-1-11-0-known-issues"></a> Known Issues

The <%= vars.product_short %> Management Console v1.11.0 has the following known issues:



#### <a id="management-console-1-11-0-vrli-https"></a> vRealize Log Insight Integration Does Not Support HTTPS Connections

**Symptom**

The <%= vars.product_short %> Management Console integration to vRealize Log Insight does not support connections to the HTTPS port on the vRealize Log Insight server.

**Workaround**

1. Use SSH to log in to the <%= vars.product_short %> Management Console appliance VM.
1. Open the file `/lib/systemd/system/pks-loginsight.service` in a text editor.
1. Add `-e LOG_SERVER_ENABLE_SSL_VERIFY=false`.
1. Set `-e LOG_SERVER_USE_SSL=true`.

    The resulting file should look like the following example:

    ```
    ExecStart=/bin/docker run --privileged --restart=always --network=pks
    -v /var/log/journal:/var/log/journal
    --name=pks-loginsight
    -e TYPE=gear2-vm
    -e LOG_SERVER_HOST=${LOGINSIGHT_HOST}
    -e LOG_SERVER_PORT=${LOGINSIGHT_PORT}
    -e LOG_SERVER_ENABLE_SSL_VERIFY=false
    -e LOG_SERVER_USE_SSL=true
    -e LOG_SERVER_AGENT_ID=${LOGINSIGHT_ID}
    pksoctopus/vrli-journald:v07092019
    ```

1. Save the file and run `systemctl daemon-reload`.
1. To restart the vRealize Log Insight service, run `systemctl restart pks-loginsight.service`.

<%= vars.product_short %> Management Console can now send logs to the HTTPS port on the vRealize Log Insight server.

#### <a id="management-console-1-11-0-vsphere-ha"></a> vSphere HA causes Management Console ovfenv Data Corruption

**Symptom**

If you enable vSphere HA on a cluster, if the TKGI Management Console appliance VM is running on a host in that cluster, and if the host reboots, vSphere HA recreates a new TKGI Management Console appliance VM on another host in the cluster. Due to an issue with vSphere HA, the `ovfenv` data for the newly created appliance VM is corrupted and the new appliance VM does not boot up with the correct network configuration.

**Workaround**

- In the vSphere Client, right-click the appliance VM and select **Power** > **Shut Down Guest OS**.
- Right-click the appliance again and select Edit Settings.
- Select **VM Options** and click **OK**.
- Verify under Recent Tasks that a `Reconfigure virtual machine` task has run on the appliance VM.
- Power on the appliance VM.

#### <a id="management-console-1-11-0-k8s-profile"></a> Base64 encoded file arguments are not decoded in Kubernetes profiles

**Symptom**

Some file arguments in Kubernetes profiles are base64 encoded. When the management console displays the Kubernetes profile,
some file arguments are not decoded.

**Workaround**

Run `echo "$content" | base64 --decode`

#### <a id="management-console-1-11-0-network-profile"></a> Network profiles not immediately selectable

**Symptom**

If you create network profiles and then try to apply them in the Create Cluster page, the new profiles
are not available for selection.

**Workaround**

Log out of the management console and log back in again.

#### <a id="management-console-1-11-0-cluster-summary"></a> Real-Time IP information not displayed for network profiles

**Symptom**

In the cluster summary page, only default IP pool, pod IP block, node IP block values are displayed,
rather than the real-time values from the associated network profile.

**Workaround**

None

#### <a id='management-console-1-11-0-harbor-storage-config'></a> Error After Modifying Your Harbor Storage Configuration

**Symptom**

You receive the following error after modifying your existing Harbor installation's storage configuration:

```
Error response from daemon: manifest for ... not found: manifest unknown: manifest unknown
```


**Explanation**

Harbor does not support modifying an existing Harbor installation's storage configuration.

**Workaround**

To modify your Harbor storage configuration,
re-install Harbor. Before starting Harbor, configure the new Harbor installation with the desired configuration.


#### <a id='management-console-1-11-0-re-import-windows-stemcell'></a> Windows Stemcells Must be Re-Imported After Upgrading Ops Manager

**Symptom**

After upgrading Ops Manager, your Management Console does not recognize a Windows stemcell imported when using the prior version of Ops Manager.

**Workaround**

After upgrading Ops Manager:

1. Re-import your previously imported Windows stemcell.  
2. **Apply Changes** to <%= vars.k8s_runtime_abbr %> MC.  

---
title: Configuring NSX-T Data Center v3.1 Edge Nodes for Tanzu Kubernetes Grid Integrated Edition
owner: TKGI-NSXT
---

This topic provides instructions for configuring NSX-T Data Center v3.1 Transport Zones and N-VDS switches on NSX Edge Nodes for use with <%= vars.product_full %> on vSphere.

##<a id='nsxt31-edge-config'></a> Transport Zone Host Switch Name Deprecation with NSX-T 3.1

When you configure an NSX 3.x Edge Node, you are presented with the opportunity to specify the **Edge Switch Name**. A messages beside this field states "The switch name value need not be identical to  host switch name associated with the Transport Zone. The use of the host switch name in Transport Zone is deprecated." The first sentence of this message does not apply to TKGI. TKGI requires that the host switch name associated with the Transport Zone must match exactly the **Edge Switch Name** value that you specify when you configure an NSX Edge Node for use with TKGI.

TKGI checks the host switch name associated with the Transport Zone during the TKGI installation process. If there is a mismatch between the the host switch name associated with the Transport Zone and **Edge Switch Name**, the istallation of <%= vars.k8s_runtime_abbr %> fails with the following error:

```
Failed to get NSX provisioning properties: No transport zone with overlay type found in transport node as switch name is not same across the TZ and ESXI TN
```

You need two transport zones for TKGI: an Overlay TZ for Transport Nodes and a VLAN TZ for Edge Nodes. You have two options: use the defeault transport zones or create custom ones.

By default NSX-T v3.0 creates two transport zones for you: `nsx-overlay-transportzone` and `nsx-vlan-transportzone`. To use the default transport zones with TKGI, when you [configure the NSX-T Edge Nodes](#nsxt30-edge-nodes), you **MUST** use the exact N-VDS switch name to be the same that is defined on the default transport zones, which is `nsxHostSwitch`. 

Alternatively, you can create custom transport zones for the Edge Nodes and use your own switch names, for example:

- `tz-overlay` (switch name: `switch-overlay`), and
- `tz-vlan` (switch name: `switch-vlan`)

The advantage of using the default transport zones is that you only need a single N-VDS for the Edge Nodes. The caveat is that you must specify this exact switch name `nsxHostSwitch`. If you are going to use the default transport zones, skip to the next section. To create custom transport zones, follow the steps below.

<p class="note"><strong>Note:</strong> If you use the default transport zones, but do not use the exact name `nsxHostSwitch` when configuring the Edge Node N-VDS switch, you will receive the  `pks-nsx-t-osb-proxy` BOSH error when you try to deploy TKGI. Refer to the <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.0/installation/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html">NSX-T documentation</a> for more information.</p>

<p class="note"><strong>Note:</strong> If you use the default transport zones, but do not use the exact name `nsxHostSwitch` when configuring the Edge Node N-VDS switch, you will receive the  `pks-nsx-t-osb-proxy` BOSH error when you try to deploy TKGI. Refer to the <a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.0/installation/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html">NSX-T documentation</a> for more information.</p>


You need two Transport Zones for <%= vars.k8s_runtime_abbr %>: an Overlay Transport Zone for Transport Nodes and a VLAN Transport Zone for Edge Nodes. You have three options:

1. Use the default Transport Zones and host switch names
1. Create custom Transport Zones using the NSX UI and get the host switch names for configuring the Edge Nodes using the NSX API
1. Create a custom Tranzport Zone and host switch name using the NSX API

| Configuration      | Transport Zone    | Host Switch Name  | 
|--------------------|-------------------|-------------------|
| Option 1: Defaults | No customization  | No customization  |
| Option 2: Hybrid   | Yes customization | No customization  |
| Option 3: API  	 | Yes customization | Yes customization |


## Option 1: Use the Default Transport Zones with a Single Default N-VDS Switch

By default NSX-T v3.0 creates two transport zones for you: `nsx-overlay-transportzone` and `nsx-vlan-transportzone`. Both default Transport Zones use a single N-VDS host switch that is named  `nsxHostSwitch`. The advantage of using the default Transport Zones is twofold. First, it simplifies the Edge Node configuration process. Second, you need only a single N-VDS for the Edge Nodes. 

To use this option, do not create any Transport Zone. When you deploy the Edge Nodes, configure a single switch, use the `nsxHostSwitch` name, and specify both default transport zones, `nsx-overlay-transportzone` and `nsx-vlan-transportzone`, in the same N-VDS configuration. 

  - If you are using the default transport zones, configure a single N-VDS switch, use the `nsxHostSwitch` name, and specify both default transport zones, `nsx-overlay-transportzone` and `nsx-vlan-transportzone`, in the same N-VDS configuration.
  - If you are using custom [transport zones](#nsxt30-tzs), configure two N-VDS switches, specify the custom transports names for each, and the corresponding custom switch name for each.

For example:

**N-VDS Switch Configuration for Default Transport Zones: One Switch**

  - **Edge Switch Name**: `nsxHostSwitch` (must match exactly)
  - **Transport Zone**: Select both the of the default transport zones `nsx-overlay-transportzone` and `nsx-vlan-transportzone`
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

<p class="note"><strong>Note:</strong> If you use the default Transport Zones, but do not use the exact name `nsxHostSwitch` when configuring the Edge Node N-VDS switch, you will receive the  `pks-nsx-t-osb-proxy` BOSH error when you try to deploy TKGI.</p>


## Option 2: Create Custom Transport Zones and Use the NSX API to Get the Host Switch Names

If you want to create a custom Transport Zone, you can do so using the NSX user interface. In this case, because the host switch name is deprecated from the NSX user interface, you have to make an NSX API call to get the host switch name so that you can configure the Edge Nodes with the correct switch. 

###<a id='nsxt31-overlay-tz'></a> Create the Custom Overlay TZ

1. In the NSX-T Management Console, navigate to **System** > **Fabric** > **Transport Zone**.
  <img src="images/nsxt/nsxt-30/tz-01.png">
1. Click **Add**.
1. Enter a Name, such as `tz-overlay`.
1. Enter a switch name, such as `switch-overlay`.
1. For the **Traffic Type**, select `Overlay`.
1. Click **Add**.
  <img src="images/nsxt/nsxt-30/tz-02.png">
1. Verify that you see the newly created TZ named `tz-overlay` in the list.
  <img src="images/nsxt/nsxt-30/tz-03.png">

###<a id='nsxt31-vlan-tz'></a> Create the Custom VLAN TZ

1. In the NSX-T Management Console, navigate to **System** > **Fabric** > **Transport Zone**.
  <img src="images/nsxt/nsxt-30/tz-01.png">
1. Click **Add**.
1. Enter a name, such as `tz-vlan`.
1. Enter a switch name, such as `switch-vlan`.
1. For the **Traffic Type**, select `VLAN`.
  <img src="images/nsxt/nsxt-30/tz-04.png">
1. Click **Add**.
1. Verify that you see the newly created TZ named `tz-vlan` in the list. 
  <img src="images/nsxt/nsxt-30/tz-05.png">

###<a id='nsxt31-hsn-api'></a> Retrieve the Host Switch Name Using the NSX API

To retrieve the host switch name, make a call to the NSX API as follows:

```
curl -k -u USER:PASSWORD -X GET "https://${NSX_MANAGER}/api/v1/transport-zones" 
```

For example:

```
$ curl -k -u user:password -X GET "https://10.20.30.40/api/v1/transport-zones"
{
  "results" : [ {
    "transport_type" : "OVERLAY",
    "host_switch_name" : "nsxHostSwitch",
    "host_switch_id" : "5bfdbfc4-c2ab-4ca7-a021-bb1fc1b45ceb",
    "transport_zone_profile_ids" : [ {
      "resource_type" : "BfdHealthMonitoringProfile",
      "profile_id" : "52035bb3-ab02-4a08-9884-18631312e50a"
    } ],
    "host_switch_mode" : "STANDARD",
    "nested_nsx" : false,
    "is_default" : true,
    "resource_type" : "TransportZone",
    "id" : "1b3a2f36-bfd1-443e-a0f6-4de01abc963e",
    "display_name" : "nsx-overlay-transportzone",
    "_create_user" : "system",
    "_create_time" : 1594850884969,
    "_last_modified_user" : "system",
    "_last_modified_time" : 1594850884969,
    "_system_owned" : false,
    "_protection" : "NOT_PROTECTED",
    "_revision" : 0,
    "_schema" : "/v1/schema/TransportZone"
  }, {
    "transport_type" : "VLAN",
    "host_switch_name" : "nsxHostSwitch",
    "host_switch_id" : "5bfdbfc4-c2ab-4ca7-a021-bb1fc1b45ceb",
    "transport_zone_profile_ids" : [ {
      "resource_type" : "BfdHealthMonitoringProfile",
      "profile_id" : "52035bb3-ab02-4a08-9884-18631312e50a"
    } ],
    "host_switch_mode" : "STANDARD",
    "nested_nsx" : false,
    "is_default" : true,
    "resource_type" : "TransportZone",
    "id" : "a95c914d-748d-497c-94ab-10d4647daeba",
    "display_name" : "nsx-vlan-transportzone",
    "_create_user" : "system",
    "_create_time" : 1594850885002,
    "_last_modified_user" : "system",
    "_last_modified_time" : 1594850885002,
    "_system_owned" : false,
    "_protection" : "NOT_PROTECTED",
    "_revision" : 0,
    "_schema" : "/v1/schema/TransportZone"
  } ],
  "result_count" : 2,
  "sort_by" : "display_name",
  "sort_ascending" : true
```


**N-VDS Configuration for Default Transport Zones**

Configure the NSX switch for the Edge Node as follows:

  - **Edge Switch Name**: Use the following switch name:
      - `nsxHostSwitch` 
  - **Transport Zone**: Select both the of the default [transport zones](#nsxt30-tzs):
      - `nsx-overlay-transportzone`
      - `nsx-vlan-transportzone`
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

**N-VDS Configuration for Custom Transport Zones**

Configure the first NSX switch for the Edge Node as follows:

  - **Edge Switch Name**:
      - `switch-overlay` (use the exact switch name that was configured for the custom transport zone `tz-overlay`)
  - **Transport Zone**:
      - `tz-overlay` (example custom transport zone)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

Configure the second NSX switch for the Edge Node as follows:

  - Click **Add Switch** (at the top of the dialog)
  - **Edge Switch Name**:
      - `switch-vlan` (use the exact switch name that was configured for the custom transport zone `tz-vlan`)
  - **Transport Zone**:
      - `tz-vlan` (example custom transport zone)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **Uplinks**: uplink-1 / EDGE-UPLINK-PG





**N-VDS Switch Configuration for Default Transport Zones: One Switch**

  - **Edge Switch Name**: `nsxHostSwitch` (must match exactly)
  - **Transport Zone**: Select both the of the default transport zones `nsx-overlay-transportzone` and `nsx-vlan-transportzone`
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

**N-VDS Switch Configuration for Default Transport Zones: Two Switches**

  - **Edge Switch Name**: `switch-overlay`, for example (must use the exact switch name that was configured for the custom transport zone)
  - **Transport Zone**: `tz-overlay`, for example (must specify the custom transport zone name)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

Configure the second NSX switch for the Edge Node as follows:

  - Click **Add Switch** (at the top of the dialog)
  - **Edge Switch Name**:
      - `switch-vlan` (use the exact switch name that was configured for the custom transport zone `tz-vlan`)
  - **Transport Zone**:
      - `tz-vlan` (example custom transport zone)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **Uplinks**: uplink-1 / EDGE-UPLINK-PG
















Option 2: Custom TZ Using UI; API Call to Get HSN
---------------------------------------------------------

1) Create custom TZ using the NSX-T UI. (You cannot customize the switch name.)
2) Retrieve the host switch name from the API.

CURL the API to get the hostwsitch. 

curl -k -u USER:PASSWORD -X GET "https://${NSX_MANAGER}/api/v1/transport-zones" -u $NSX_USER

3) Create the Edge Nodes to use those names.

Option 3: Use API to Create Custom TZ and Custom HSN
---------------------------------------------------------------------

1. Create TZ from the API. You can customize the switch name.
2. Retrieve host swtich name using API. 

Here is a slack thread with info on how to do it: https://vmware.slack.com/archives/CD8AFHNP5/p1612459932086000

 Refererence:

https://github.com/pivotal-cf/docs-pks/issues/1169

 

##<a id='nsxt30-tzs'></a> Create Transport Zones

You need two transport zones for TKGI: an Overlay TZ for Transport Nodes and a VLAN TZ for Edge Nodes. You have two options: use the defeault transport zones or create custom ones.

By default NSX-T v3.0 creates two transport zones for you: `nsx-overlay-transportzone` and `nsx-vlan-transportzone`. To use the default transport zones with TKGI, when you [configure the NSX-T Edge Nodes](#nsxt30-edge-nodes), you **MUST** use the exact N-VDS switch name to be the same that is defined on the default transport zones, which is `nsxHostSwitch`. 

Alternatively, you can create custom transport zones for the Edge Nodes and use your own switch names, for example:

- `tz-overlay` (switch name: `switch-overlay`), and
- `tz-vlan` (switch name: `switch-vlan`)



###<a id='nsxt30-overlay-tz'></a> Create the Overlay TZ

1. In the NSX-T Management Console, navigate to **System** > **Fabric** > **Transport Zone**.
  <img src="images/nsxt/nsxt-30/tz-01.png">
1. Click **Add**.
1. Enter a Name, such as `tz-overlay`.
1. Enter a switch name, such as `switch-overlay`.
1. For the **Traffic Type**, select `Overlay`.
1. Click **Add**.
  <img src="images/nsxt/nsxt-30/tz-02.png">
1. Verify that you see the newly created TZ named `tz-overlay` in the list.
  <img src="images/nsxt/nsxt-30/tz-03.png">

###<a id='nsxt30-vlan-tz'></a> Create the VLAN TZ

1. In the NSX-T Management Console, navigate to **System** > **Fabric** > **Transport Zone**.
  <img src="images/nsxt/nsxt-30/tz-01.png">
1. Click **Add**.
1. Enter a name, such as `tz-vlan`.
1. Enter a switch name, such as `switch-vlan`.
1. For the **Traffic Type**, select `VLAN`.
  <img src="images/nsxt/nsxt-30/tz-04.png">
1. Click **Add**.
1. Verify that you see the newly created TZ named `tz-vlan` in the list. 
  <img src="images/nsxt/nsxt-30/tz-05.png">


In this section you deploy two NSX-T Edge Nodes. 

NSX Edge Nodes provide the bridge between the virtual network environment implemented using NSX-T and the physical network. Edge Nodes for <%= vars.product_short %> run load balancers for <%= vars.control_plane %> traffic, Kubernetes load balancer services, and ingress controllers. See [Load Balancers in <%= vars.product_short %>](./about-lb.html) for more information.

In NSX-T, a load balancer is deployed on the Edge Nodes as a virtual server. The following virtual servers are required for <%= vars.product_short %>:

- 1 TCP Layer 4 virtual server for each Kubernetes service of type:`LoadBalancer`
- 2 Layer 7 global virtual servers for Kubernetes pod ingress resources (HTTP and HTTPS)
- 1 global virtual server for the <%= vars.control_plane %>

The number of virtual servers that can be run depends on the size of the load balancer which depends on the size of the Edge Node. <%= vars.product_short %> supports the `medium` and `large` VM Edge Node form factor, as well as the bare metal Edge Node. The default size of the load balancer deployed by NSX-T for a Kubernetes cluster is `small`. The size of the load balancer can be customized using <a href="./network-profiles-define.html">Network Profiles</a>. 

For this installation, we use the Large VM form factor for the Edge Node. See [VMware Configuration Maximums](https://configmax.vmware.com/guest?vmwareproduct=VMware%20NSX-T&release=NSX-T%20Data%20Center%203.0.0&categories=17-0) for more information.

###<a id='nsxt30-edge-node-1'></a> Install Edge Node 1 

Deploy the Edge Node 1 VM using the NSX-T Manager interface.

1. From your browser, log in with admin privileges to NSX Manager at `https://NSX-MANAGER-IP-ADDRESS`.

1. In NSX Manager, go to **System** > **Fabric** > **Nodes** > **Edge Transport Nodes**.
  <img src="images/nsxt/nsxt-30/edge-node-01.png">

1. Click **Add Edge VM**.

1. Configure the Edge VM as follows:
  - **Name**: `edge-node-1`
  - **Host name/FQDN**: `edge-node-1.lab.com`
  - **Form Factor**: **Large**
  <img src="images/nsxt/nsxt-30/edge-node-02.png">

1. Configure **Credentials** as follows:
  - **CLI User Name**: `admin`
  - **CLI Password**: Enter a strong password for the `admin` user that complies with the NSX-T requirements.
  - **Enable SSH Login**: Yes
  - **System Root Password**: Enter a strong password for the `root` user that complies with the NSX-T requirements.
  - **Enable Root SSH Login**: Yes
  - **Audit Credentials**: Enter an `audit` user name and password.
  <img src="images/nsxt/nsxt-30/edge-node-03.png">

1. Configure the deployment as follows:
  - **Compute Manager**: vCenter
  - **Cluster**: MANAGEMENT-Cluster
  - **Datastore**: Select the datastore
  <img src="images/nsxt/nsxt-30/edge-node-04.png">

1. Configure the node settings as follows:
  - **IP Assignment**: Static
  - **Management IP**: 10.173.62.49/24, for example
  - **Default Gateway**: 10.173.62.253, for example
  - **Management Interface**: PG-MGMT-VLAN-1548, for example
  <img src="images/nsxt/nsxt-30/edge-node-05.png">

####<a id='nsxt30-edge-node-1-nvds'></a> Configure the N-VDS Switch

The next step is to configure the N-VDS switch and transport zones for the NSX Edge Node. 

How you do this differs depending on if you are using the default or custom [transport zones](#nsxt30-tzs). 
- If you are using the default [transport zones](#nsxt30-tzs), configure a single switch, use the `nsxHostSwitch` name, and specify both default transport zones, `nsx-overlay-transportzone` and `nsx-vlan-transportzone`, in the same N-VDS configuration.
- If you are using custom [transport zones](#nsxt30-tzs), configure two N-VDS switches, specify the custom transports names for each, and the corresponding custom switch name for each.

**N-VDS Configuration for Default Transport Zones**

Configure the NSX switch for the Edge Node as follows:

  - **Edge Switch Name**: Use the following switch name:
  		- `nsxHostSwitch` 
  - **Transport Zone**: Select both the of the default [transport zones](#nsxt30-tzs):
    	- `nsx-overlay-transportzone`
    	- `nsx-vlan-transportzone`
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

**N-VDS Configuration for Custom Transport Zones**

Configure the first NSX switch for the Edge Node as follows:

  - **Edge Switch Name**:
    	- `switch-overlay` (use the exact switch name that was configured for the custom transport zone `tz-overlay`)
  - **Transport Zone**:
    	- `tz-overlay` (example custom transport zone)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **IP Assignment**: Use IP Pool
  - **IP Pool**: TEP-IP-POOL
  - **Uplinks**: uplink-1 / EDGE-VTEP-PG

Configure the second NSX switch for the Edge Node as follows:

  - Click **Add Switch** (at the top of the dialog)
  - **Edge Switch Name**:
    	- `switch-vlan` (use the exact switch name that was configured for the custom transport zone `tz-vlan`)
  - **Transport Zone**:
    	- `tz-vlan` (example custom transport zone)
  - **Uplink Profile**: `nsx-edge-single-nic-uplink-profile`
  - **Uplinks**: uplink-1 / EDGE-UPLINK-PG

####<a id='nsxt30-edge-node-1-complete'></a> Complete the Edge Node 1 Installation

1. Click **Finish** to complete the configuration. The installation begins.

1. In vCenter, use the **Recent Tasks** panel at the bottom of the page to verify that you see the Edge Node 1 VM being deployed.

1. Once the process completes, you should see the Edge Node 1 deployed successfully in NSX-T Manager.
  <img src="images/nsxt/nsxt-30/edge-node-08.png">

1. Click the N-VDS link and verify that you see both switches.
  <img src="images/nsxt/nsxt-30/edge-node-09.png">

1. In vCenter verify that the Edge Node is created.
  <img src="images/nsxt/nsxt-30/edge-node-10.png">

###<a id='nsxt30-edge-node-2'></a> Install Edge Node 2

Repeat the same operation for Edge Node 2, and for each additional NSX Edge Node pair you intend to use for <%= vars.product_short %>. 

1. Install `nsx-edge-2` following the same procedure as `nsx-edge-1`.
  - name: `edge-node-2`
  - hostname/FQDN: `edge-node-2.lab.com`, for example
  - Form Factor: Large
  - IP Assignment: Static
  - IP: 10.173.62.58/24, for example
  - GW: 10.173.62.253, , for example
  - Management Interface: `PG-MGMT-VLAN-1548`
  - Edge Switch 1: Name: 
      - `nsxHostSwitch` (if you are using the default TZs) 
      - `switch-overlay` (if you are using a custom TZ)
  - Edge Switch 1: Transport Zone: 
      - `nsx-overlay-transportzone` and `nsx-vlan-transportzone` (if you are using the default TZs)
      - `tz-overlay` (if you are using a custom TZ)
    - Uplink Profile: `nsx-edge-single-nic-uplink-profile`
    - IP Assignment: `Use IP Pool`
    - IP Pool: `TEP-IP-POOL`
    - Uplinks: `uplink-1` / `EDGE-VTEP-PG`
  - Edge Switch 2: (only required if you are using a custom TZ)
      - Name: `switch-vlan` (use the same switch name that was configured for tz-vlan)
      - Transport Zone: `tz-vlan`
      - Uplink Profile: `nsx-edge-single-nic-uplink-profile`
      - Uplinks: `uplink-1` / `EDGE-UPLINK-PG`

1. Once done, you should be able to see both Edge Nodes in NSX Manager.
  <img src="images/nsxt/nsxt-30/edge-node-11.png">

##<a id='nsxt30-uplink-profile'></a> Create Uplink Profile for ESXi Transport Node

To configure the TEP, we used the default profile named `nsx-default-uplink-hostswitch-profile`. However, because the TEP is on VLAN 3127, you must modify the uplink profile for the ESXI Transport Node (TN). NSX-T does not allow you to edit settings the default uplink profile, so we create a new one.

1. Go to **System** > **Fabric** > **Profiles** > **Uplink Profiles**.
  <img src="images/nsxt/nsxt-30/uplink-profile-01.png">
 
1. Click **Add**.

1. Configure the **New Uplink Profile** as follows:
  - **Name**: `nsx-esxi-uplink-hostswitch-profile`
  - **Teaming Policy**: `Failover Order` 
  - **Active Uplinks**: `uplink-1`
  - **Transport vLAN**: `3127`
  <img src="images/nsxt/nsxt-30/uplink-profile-02.png">

1. Click **Add**.

1. Verify that the **Uplink Profile** is created.
  <img src="images/nsxt/nsxt-30/uplink-profile-03.png">

##<a id='nsxt30-esxi-tn'></a> Deploy ESXi Host Transport Nodes Using VDS

Deploy each ESXi host in the COMPUTE-cluster as an ESXi host transport node (TN) in NSX-T. If you have not created a separate COMPUTE-cluster for ESXi hosts, deploy each ESXi host in the vSphere cluster as a host transport node in NSX-T.

1. Go to **System** > **Fabric** > **Nodes** > **Host Transport Nodes**.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-01.png">

1. Expand the Compute Manager and select the ESXi host in the COMPUTE-cluster, or each ESXi host in the vSphere cluster.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-02.png">
 
1. Click **Configure NSX**.

1. In the **Host Details** tab, enter a name, such as `10.172.210.57`.

1. In the **Configure NSX** tab, configure the transport node as follows:
  - **Type**: `VDS` (do not select the N-VDS option)
  - **Name**: `switch-overlay` (you must use the same switch name that was configured for `tz-overlay` transport zone)
  - **Transport Zone**: `tz-overlay`
  - **NIOC Profile**: `nsx-default-nioc-hostswitch-profile`
  - **Uplink Profile**: `nsx-esxi-uplink-hostswitch-profile`
  - **LLDP Profile**: `LLDP [Send Packet Disabled]`
  - **IP Assignment**: `Use IP Pool`
  - **IP Pool**: `TEP-IP-POOL`
  - **Teaming Policy Switch Mapping**
     - **Uplinks**: `uplink-1`
     - **Physical NICs**: `vmnic1`
  <img src="images/nsxt/nsxt-30/esxi-host-tn-33.png">

1. Click **Finish**.

1. Verify that the host TN is configured.
  <img src="images/nsxt/nsxt-30/esxi-host-tn-04.png">

##<a id='nsxt30-check-tep'></a> Verify TEP to TEP Connectivity

To avoid any overlay communication in the future due to MTU issue, test TEP to TEP connectivity and verify that it is working.

1. SSH to edge-node-1 and get the local TEP IP address, such as `192.23.213.1`. Use the command `get vteps` to get the IP.

1. SSH to edge-node-2 and get the local TEP IP address, ushc as `192.23.213.2`. Use the command `get vteps` to get the IP.

1. SSH to the ESXi host and get the TEP IP address, such as `192.23.213.3`. Use the command `esxcfg-vmknic -l` to get the IP. The interface will be `vmk10` and the NetStack will be `vxlan`.

1. From each ESXi transport node, test the connections to each NSX-T Edge Node, for example:

    ```
    # vmkping ++netstack=vxlan 192.23.213.1 -d -s 1572 -I vmk10: OK
    # vmkping ++netstack=vxlan 192.23.213.2 -d -s 1572 -I vmk10: OK
    ```

    1. Test the connection from NSX-T edge node 1 and edge node 2 to ESXi TN:

        ```
        > vrf 0
        > ping 192.23.213.1 size 1572 dfbit enable: OK
        ```

    1. Test the connection from NSX-T edge node 1 to NSX-T edge node 2:

        ```
        > vrf 0
        > ping 192.23.213.2 size 1572 dfbit enable: OK
        ```

##<a id='nsxt30-edge-cluster'></a> Create NSX-T Edge Cluster

1. Go to **System** > **Fabric** > **Nodes** > **Edge Clusters**.
  <img src="images/nsxt/nsxt-30/edge-cluster-01.png">
 
1. Click **Add**.
  - Enter a name, such as `edge-cluster-1`.
  - Add members, including `edge-node-1` and `edge-node-2`.
  <img src="images/nsxt/nsxt-30/edge-cluster-02.png">

1. Click **Add**.

1. Verify.
  <img src="images/nsxt/nsxt-30/edge-cluster-03.png">

##<a id='nsxt30-uplink-ls'></a> Create Uplink Logical Switch

Create an uplink Logical Switch to be used for the Tier-0 Router.

1. At upper-right, select the **Manager** tab.

1. Go to **Networking** > **Logical Switches**.
  <img src="images/nsxt/nsxt-30/uplink-ls-01.png">

1. Click **Add**.

1. Configure the new logical switch as follows: 
  - Name: `LS-T0-uplink`
  - Transport Zone: `tz-vlan`
  - VLAN: `1548`
  <img src="images/nsxt/nsxt-30/uplink-ls-02.png">

1. Click **Add**.

1. Verify.
  <img src="images/nsxt/nsxt-30/uplink-ls-03.png">

##<a id='nsxt30-t0-router-create'></a> Create Tier-0 Router

1. Select **Networking** from the **Manager** tab.
  <img src="images/nsxt/nsxt-30/tier-0-01.png">
 
1. Select **Tier-0 Logical Router**.
  <img src="images/nsxt/nsxt-30/tier-0-02.png">
 
1. Click **Add**.

1. Configure the new Tier-0 Router as follows:
  - **Name**: `T0-router`
  - **Edge Cluster**: `edge-cluster-1`
  - **HA mode**: `Active-Standby`
  - **Failover mode**: `Non-Preemptive`
  <img src="images/nsxt/nsxt-30/tier-0-03.png">

1. Click **Save** and verify.
  <img src="images/nsxt/nsxt-30/tier-0-04.png">
 
1. Select the T0 router.
  <img src="images/nsxt/nsxt-30/tier-0-05.png">

1. Select **Configuration** > **Router Ports**.
 
1. Click **Add**.

1. Configure a new router port as follows: 
  - **Name**: T0-uplink-1
  - **Type**: uplink
  - **Transport Node**: edge-node-1
  - **Logical Switch**: LS-T0-uplink
  - **Logical Switch Port**: Attach to a new switch port
  - **Subnet**: 10.173.62.50 / 24
  <img src="images/nsxt/nsxt-30/tier-0-06.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/tier-0-07.png">

1. Select the T0 router.

1. Select **Configuration** > **Router Ports**.

1. Add a second uplink by creating a second router port for edge-node-2:
  - **Name**: T0-uplink-1
  - **Type**: uplink
  - **Transport Node**: edge-node-2
  - **Logical Switch**: LS-T0-uplink
  - **Logical Switch Port**: Attach to a new switch port
  - **Subnet**: 10.173.62.51 / 24

1. Once completed, verify that you have two connected router ports.
  <img src="images/nsxt/nsxt-30/tier-0-08.png">

##<a id='nsxt30-t0-router-config'></a> Configure and Test the Tier-0 Router

Create an HA VIP for the T0 router, and a default route for the T0 router. Then test the T0 router.

1. Select the Tier-0 Router you created.

1. Select **Configuration** > **HA VIP**.

1. Click **Add**.
  <img src="images/nsxt/nsxt-30/tier-0-09.png">

1. Configure the HA VIP as follows:
  - **VIP address**: 10.173.62.52/24, for example
  - **Uplink ports**: T0-uplink-1 and T0-uplink-2
  <img src="images/nsxt/nsxt-30/tier-0-10.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/tier-0-11.png">

1. Select **Routing** > **Static Routes**.
  <img src="images/nsxt/nsxt-30/tier-0-12.png">

1. Click **Add**.
  - **Network**: `0.0.0.0/0`
  - **Next Hop**: `10.173.62.253`
  <img src="images/nsxt/nsxt-30/tier-0-13.png">

1. Click **Add** and verify. 
  <img src="images/nsxt/nsxt-30/tier-0-14.png">

1. Verify the Tier 0 router by making sure the T0 uplinks and HA VIP are reachable from your laptop.

For example:

```
> ping 10.173.62.50
PING 10.173.62.50 (10.173.62.50): 56 data bytes
Request timeout for icmp_seq 0
64 bytes from 10.173.62.50: icmp_seq=1 ttl=58 time=71.741 ms
64 bytes from 10.173.62.50: icmp_seq=0 ttl=58 time=1074.679 ms

> ping 10.173.62.51
PING 10.173.62.51 (10.173.62.51): 56 data bytes
Request timeout for icmp_seq 0
64 bytes from 10.173.62.51: icmp_seq=0 ttl=58 time=1156.627 ms
64 bytes from 10.173.62.51: icmp_seq=1 ttl=58 time=151.413 ms

> ping 10.173.62.52
PING 10.173.62.52 (10.173.62.52): 56 data bytes
64 bytes from 10.173.62.52: icmp_seq=0 ttl=58 time=6.864 ms
64 bytes from 10.173.62.52: icmp_seq=1 ttl=58 time=7.776 ms
```

##<a id='nsxt30-ip-blocks-pool'></a> Create IP Blocks and Pool for Compute Plane

TKGI requires a Floating IP Pool for NSX-T load balancer assignment and the following 2 IP blocks for Kubernetes pods and nodes:

- PKS-POD-IP-BLOCK: 172.16.0.0/16
- PKS-NODE-IP-BLOCK: 172.23.0.0/16 

1. In the **Manager** interface, go to **Networking** > **IP Address Pools** > **IP Block**.
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-01.png">

1. Click **Add**.

1. Configure the Pod IP Block as follows:
  - **Name**: PKS-POD-IP-BLOCK
  - **CIDR**: 172.16.0.0/16
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-02.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-03.png">

1. Repeat same operation for the Node IP Block.
  - **Name**: PKS-NODE-IP-BLOCK
  - **CIDR**: 172.23.0.0/16

1. Click **Add** and verify. 
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-04.png">

1. Select **IP Pools** tab.
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-05.png">

1. Click **Add**.

1. Configure the IP pool as follows: 
  - **Name**: PKS-FLOATING-IP-POOL
  - **IP ranges**: 10.173.62.111 - 10.173.62.150
  - **CIDR**: 10.173.62.0/24
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-06.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/ib-blocks-pool-07.png">

##<a id='nsxt30-mgmt-plane'></a> Create Management Plane

Networking for the <%= vars.k8s_runtime_abbr %> Management Plane consists of a [Tier-1 Router and Switch](#nsxt30-t1-router) with [NAT Rules](#nsxt30-t0-nat) for the Management Plane VMs.

###<a id='nsxt30-t1-router'></a> Create Tier-1 Router and Switch 

Create Tier-1 Logical Switch and Router for TKGI Management Plane VMs. Complete the configuration by enabling Route Advertisement on the T1 router.

1. In the NSX Management console, navigate to **Networking > Logical Switches**.

1. Click **Add**.

1. Create the LS for TKGI Management plane VMs:
  - **Name**: LS-PKS-MGMT
  - **Transport Zone**: tz-overlay
  <img src="images/nsxt/nsxt-30/tier-1-01.png">

1. Click **Add** and verify creation of the T1 logical switch.
  <img src="images/nsxt/nsxt-30/tier-1-02.png">

1. Go to **Networking** > **Tier-1 Logical Router**.
  <img src="images/nsxt/nsxt-30/tier-1-03.png">

1. Click **Add**.

1. Configure the Tier-1 logical router as follows:
  - **Name**: T1-PKS-MGMT
  - **To router**: T0-router
  - **Edge Cluster**: edge-cluster-1
  - **Edge Cluster Members**: edge-node-1 and edge-node-2
  <img src="images/nsxt/nsxt-30/tier-1-04.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/tier-1-05.png">

1. Select the T1 router and go to **Configuration** > **Router port**.
  <img src="images/nsxt/nsxt-30/tier-1-06.png">

1. Click **Add**.

1. Configure the T1 router port as follows:
  - **Name**: T1-PKS-MGMT-port
  - **Logical Switch**: LS-PKS-MGMT
  - **Subnet**: 10.1.1.1/24
  <img src="images/nsxt/nsxt-30/tier-1-07.png">

1. Click **Add** and verify.
  <img src="images/nsxt/nsxt-30/tier-1-08.png">

1. Select **Routing** tab.
  <img src="images/nsxt/nsxt-30/tier-1-09.png">

1. Click **Edit** and configure route advertisement as follows:
  - **Status**: Enabled
  - **Advertise All Connected Routes**: Yes
  <img src="images/nsxt/nsxt-30/tier-1-10.png">

1. Click **Save** and verify.
  <img src="images/nsxt/nsxt-30/tier-1-11.png">

###<a id='nsxt30-t0-nat'></a> Create NAT Rules 

You need to create the following NAT rules on the Tier-0 router for the TKGI Management Plane VMs.

- DNAT: `10.173.62.220` (for example) to access Ops Manager
- DNAT: `10.173.62.221` (for example) to access Harbor
- SNAT: `10.173.62.222` (for example) for all TKGI management plane VM traffic destined to the outside world

1. In the NSX Management console, navigate to **Networking** > **NAT**.

1. In the Logical Router field, select the T0-router you defined for TKGI.
  <img src="images/nsxt/nsxt-30/tier-0-nat-01.png">

1. Click **Add**.

1. Configure the Ops Manager DNAT rule as follows:
  - **Priority**: `1000`
  - **Action**: `DNAT`
  - **Protocol**: `Any Protocol`
  - **Destination IP**: `10.173.62.220`, for example
  - **Translated IP**: `10.1.1.2`, for example
  <img src="images/nsxt/nsxt-30/tier-0-nat-02.png">

1. Click **Add** and verify. 
  <img src="images/nsxt/nsxt-30/tier-0-nat-03.png">

1. Add a second DNAT rule for Harbor by repeating the same operation.
  - **Priority**: `1000`
  - **Action**: `DNAT`
  - **Protocol**: `Any Protocol`
  - **Destination IP**: `10.173.62.221`, for example
  - **Translated IP**: `10.1.1.6`, for example

1. Verify the creation of the DNAT rules.
  <img src="images/nsxt/nsxt-30/tier-0-nat-04.png">

1. Create the SNAT rule for the management plane traffic as follows:
  - **Priority**: `9024`
  - **Action**: `SNAT`
  - **Protocol**: `Any Protocol`
  - **Source IP**: `10.1.1.0/24`, for example
  - **Translated IP**: `10.173.62.222`, for example
    <img src="images/nsxt/nsxt-30/tier-0-nat-05.png">

1. Verify the creation of the SNAT rule. 
  <img src="images/nsxt/nsxt-30/tier-0-nat-06.png">

##<a id='nsxt-password-mgr'></a> Configure the NSX-T Password Interval (Optional)

The default NSX-T password expiration interval is 90 days. After this period, the NSX-T passwords will expire on all NSX-T Manager Nodes and all NSX-T Edge Nodes. To avoid this, you can extend or remove the password expiration interval, or change the password if needed.

<p class="note"><strong>Note:</strong> For existing <%= vars.product_short %> deployments, anytime the NSX-T password is changed you must update the BOSH and PKS tiles with the new passwords. See <a href="./password-management.html">Adding Infrastructure Password Changes to the <%= vars.product_short %> Tile</a> for more information.</p>

###<a id='nsxt-manager-password'></a> Update the NSX-T Manager Password and Password Interval

To update the NSX Manager password, perform the following actions on **one** of the NSX Manager nodes. The changes will be propagated to all NSX Manager nodes.

#### SSH into the NSX Manager Node

To manage user password expiration, you use the CLI on one of the NSX Manager nodes. 

To access a NSX Manager node, from Unix hosts use the command `ssh USERNAME@IP_ADDRESS_OF_NSX_MANAGER`. 

For example:

```
ssh admin@10.196.188.22
```

On Windows, use Putty and provide the IP address for NSX Manager. Enter the user name and password that you defined during the installation of NSX-T.

#### Retrieve the Password Expiration Interval

To retrieve the password expiration interval, use the following command:

[get user USERNAME password-expiration](https://vdc-download.vmware.com/vmwb-repository/dcr-public/0aa6ceb3-9f71-44f5-bef6-d4acab570b55/382a3731-d45e-43c1-ba0b-f98b64899e9c/nsxt_24_cli.html#get%20user%20%3Cusername%3E%20password-expiration)

For example:

```
NSX CLI (Manager, Policy, Controller 3.0.0.0.0.15946739). Press ? for cost or enter: help
nsx-mgr-1> get user admin password-expiration
Password expires 90 days after last change
```

#### Update the Admin Password

To update the user password, use the following command:

[set user USERNAME password NEW-PASSWORD old-password OLD-PASSWORD](https://vdc-download.vmware.com/vmwb-repository/dcr-public/0aa6ceb3-9f71-44f5-bef6-d4acab570b55/382a3731-d45e-43c1-ba0b-f98b64899e9c/nsxt_24_cli.html#set%20user%20%3Cusername%3E%20[password%20%3Cpassword%3E%20[old-password%20%3Cold-password%3E]]).

For example: 

```
set user admin password my-new-pwd old-password my-old-pwd
```

#### Set the Admin Password Expiration Interval

To set the password expiration interval, use the following command:

[set user USERNAME password-expiration PASSWORD-EXPIRATION](https://vdc-download.vmware.com/vmwb-repository/dcr-public/0aa6ceb3-9f71-44f5-bef6-d4acab570b55/382a3731-d45e-43c1-ba0b-f98b64899e9c/nsxt_24_cli.html#set%20user%20%3Cusername%3E%20password-expiration%20%3Cpassword-expiration%3E).

For example, the following command sets the password expiration interval to 120 days: 

```
set user admin password-expiration 120
```

#### Remove the Admin Password Expiration Interval

To remove password expiration, use the following command:

[clear user USERNAME password-expiration](https://vdc-download.vmware.com/vmwb-repository/dcr-public/0aa6ceb3-9f71-44f5-bef6-d4acab570b55/382a3731-d45e-43c1-ba0b-f98b64899e9c/nsxt_24_cli.html#clear%20user%20%3Cusername%3E%20password-expiration).

For example: 

```
clear user admin password-expiration
```

To verify:

```
nsx-mgr-1> clear user admin password-expiration
nsx-mgr-1> get user admin password-expiration
Password expiration not configured for this user
```

###<a id='nsxt-password-edge'></a> Update the Password for NSX Edge Nodes

To update the NSX Edge Node password, perform the following actions on **each** NSX Edge Node.

<p class="note"><strong>Note:</strong> Unlike the NSX-T Manager nodes, you must update the password or password interval on each Edge Node.</p>

#### Enable SSH

SSH on the Edge Node is disabled by default. You have to enable SSH on the Edge Node using the Console from vSphere.

```
start service ssh
set service ssh start-on-boot
```

#### SSH to the NSX Edge Node

For example:

```
ssh admin@10.196.188.25
```

#### Get the Password Expiration Interval for the Edge Node

For example:

```
nsx-edge> get user admin password-expiration
Password expires 90 days after last change
```

#### Update the User Password for the Edge Node

For example: 

```
nsx-edge> set user admin password my-new-pwd old-password my-old-pwd
```

#### Set the Password Expiration Interval

For example, the following command sets the password expiration interval to 120 days: 

```
nsx-edge> set user admin password-expiration 120
```

#### Remove the Password Expiration Interval

For example: 

```
NSX CLI (Edge 3.0.0.0.0.15946012). Press ? for command list or enter: help
nsx-edge-2> get user admin password-expiration
Password expires 90 days after last change. Current password will expire in 7 days.

nsx-edge-2> clear user admin password-expiration
nsx-edge-2> get user admin password-expiration
Password expiration not configured for this user
```

##<a id='next'></a> Next Steps

Once you have completed the installation of NSX-T v3.0, return to the TKGI installation workflow and proceed with the next phase of the process. See [Install Tanzu Kubernetes Grid Integrated Edition on vSphere with NSX-T Using Ops Manager](./vsphere-nsxt-index.html).
